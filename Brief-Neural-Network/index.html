<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=0" name="viewport"><meta name="keywords" content=""><meta name="description" content="Sshwy 的个人博客"><meta name="generator" content="Hexo 6.3.0"><title>神经网络入门 - Sshwy&#39;s Notes</title><link rel="shortcut icon" href="/images/favicon.ico"><link rel="stylesheet" href="/css/ssimple.css?20250423.css"><link rel="stylesheet" href="/iconfont/iconfont.css?20250423.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/katex.min.css"><script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/katex.min.js" crossorigin="anonymous"></script><script>document.addEventListener("DOMContentLoaded",function(){window.renderKatex()})</script></head><body><header><div class="header" style="width:100%"><div class="header-inner" style="position:fixed;z-index:20;top:0;left:0"><div class="header-content" style="max-width:1200px;margin:auto"><div class="blog-title"><span class="iconfont icon-menu1" id="menu-button"></span> <a href="/" class="logo header-title">Sshwy&#39;s Notes</a><div class="mobile-search"><input type="text"> <span class="iconfont icon-search mobile-search-icon"></span> <span class="iconfont icon-baseline-close-px search-close-icon"></span></div></div><div class="navbar" id="menu-list"><ul class="menu"><li class="menu-item"><a href="/archives/" class="menu-item-link"><span class="menu-item-icon iconfont icon-work"></span> Archives</a></li><li class="menu-item"><a href="/directory/" class="menu-item-link"><span class="menu-item-icon iconfont icon-folder-close"></span> Directory</a></li><li class="menu-item"><a href="/about/" class="menu-item-link"><span class="menu-item-icon iconfont icon-user"></span> About</a></li><li class="menu-item"><a href="/static/beibishi2023" class="menu-item-link"><span class="menu-item-icon iconfont icon-favorite"></span> NOI 背笔试</a></li></ul></div><div class="search"><input type="text"> <span class="iconfont icon-search search-icon"></span> <span class="iconfont icon-baseline-close-px search-close-icon"></span></div></div></div><div class="search-shadow"></div><div class="search-box"><div class="search-container"><div class="search-container-inner"><div class="search-data-status"><span>Fetching search data...</span></div><div class="search-count"></div><div class="search-result"></div></div></div></div></div></header><main class="main"><article class="post"><div class="post-title"><h1 class="page-title">神经网络入门</h1></div><div class="post-meta"><div class="post-info"><span class="post-info-item post-time" title="2023年11月25日星期六晚上8点26分 (CST+08:00)"><span class="info-icon iconfont icon-time"></span>更新于 2023年11月25日 </span><span class="icon infosep"></span><span class="post-info-item post-wordcount"> <span class="info-icon iconfont icon-text"></span>约 8,116 字</span></div><div class="post-directory"><span class="iconfont icon-folder-close"></span> <a class="directory" href="/directory">Home</a> <a class="directory" href="/directory/"></a> <span class="icon smallarrow"></span> 当前文章</div><div class="post-tags"><span class="tag"><span class="meta-icon iconfont icon-tag"></span><a class="tag" href="/tags/Notes/">Notes</a></span></div><hr></div><div class="toc"><h2 class="toc-title"><span class="iconfont icon-explain" style="font-size:1em;padding-right:5px"></span>文章目录</h2><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#向量"><span class="toc-number">1.</span> <span class="toc-text">向量</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#神经网络概述"><span class="toc-number">2.</span> <span class="toc-text">神经网络概述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#学习与误差"><span class="toc-number">3.</span> <span class="toc-text">学习与误差</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#神经网络"><span class="toc-number">4.</span> <span class="toc-text">神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#线性层"><span class="toc-number">4.1.</span> <span class="toc-text">线性层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#激活层"><span class="toc-number">4.2.</span> <span class="toc-text">激活层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#卷积层"><span class="toc-number">4.3.</span> <span class="toc-text">卷积层</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#反向传播概述"><span class="toc-number">5.</span> <span class="toc-text">反向传播概述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#误差函数"><span class="toc-number">6.</span> <span class="toc-text">误差函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#反向传播"><span class="toc-number">7.</span> <span class="toc-text">反向传播</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#线性层-1"><span class="toc-number">7.1.</span> <span class="toc-text">线性层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#激活层-1"><span class="toc-number">7.2.</span> <span class="toc-text">激活层</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#梯度下降"><span class="toc-number">8.</span> <span class="toc-text">梯度下降</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#分批处理数据（batch）"><span class="toc-number">8.1.</span> <span class="toc-text">分批处理数据（batch）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#随机梯度下降"><span class="toc-number">8.2.</span> <span class="toc-text">随机梯度下降</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#自适应梯度下降"><span class="toc-number">8.3.</span> <span class="toc-text">自适应梯度下降</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#代码实现"><span class="toc-number">9.</span> <span class="toc-text">代码实现</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#NumPy-入门"><span class="toc-number">9.1.</span> <span class="toc-text">NumPy 入门</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Broadcasting1"><span class="toc-number">9.1.1.</span> <span class="toc-text">Broadcasting1</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Indexing2"><span class="toc-number">9.1.2.</span> <span class="toc-text">Indexing2</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#指南"><span class="toc-number">9.2.</span> <span class="toc-text">指南</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Norms"><span class="toc-number">10.</span> <span class="toc-text">Norms</span></a></li></ol></div><hr></div><div class="post-content"><p>最近有好多朋友受困于手写神经网络的作业，因此我将自己的学习思路总结成本文，希望能提供一定的帮助～</p><h2><a id="向量" href="#向量" class="headerlink" title="向量"></a>向量</h2><p>在神经网络中，高维向量（vector）和张量（tensor）没有太大区别。从代码层面，两者的实现都是一个高维数组。</p><p>引入向量的原因是，神经网络解决的问题往往不是数字到数字的映射，而是一个事物到另一个事物的变换。在写代码时我们可以用结构体或者类来表示一个对象，但这样的编程范式和语法设计主要是为了方便人的阅读。而神经网络读不懂这些。另一方面，数学上定义一个对象通常采用元组（tuple）和序列（sequence）来描述，例如我们说图灵机是一个七元组<script type="math/tex">M = \langle Q, \Gamma, b, \Sigma, \delta, q_0, F \rangle</script>。而元组可以粗暴理解为长度固定的序列，也就是向量。</p><p>因此，神经网络只认向量，我们需要将信息转化为向量后传递给网络。</p><h2><a id="神经网络概述" href="#神经网络概述" class="headerlink" title="神经网络概述"></a>神经网络概述</h2><p>神经网络本身是一个函数。它接受一个向量作为输入， 返回一个向量作为输出。具体是什么形状的输入向量和什么形状的输出向量，看你怎么设计。</p><p>以手写数字识别为例：数据集是由大量<script type="math/tex">28\times 28</script>的灰度（只有黑白灰）图片组成。如果神经网络的第一层是一个线性层（linear，也称全连接 full connected，或者稠密层，dense），那么我们传递的就是一个长度为<script type="math/tex">28 * 28 = 784</script>的一维向量。而如果第一层是卷积层（convolutional），那我们传递的就是一个<script type="math/tex">28 \times 28</script>的二维向量。</p><p>在手写数字识别这个问题中，输出向量的形状大多是长度为<script type="math/tex">10</script>的一维向量。但是这个向量的含义也是不唯一的。比如说你可以利用 softmax 函数让神经网络直接输出一个离散概率分布（即输出一个向量<script type="math/tex">p</script>，其中<script type="math/tex">x = i</script>的概率是<script type="math/tex">p_i</script>），也可以让神经网络直接输出一个向量表示每个数字的权重，然后你再自己根据权重分析出神经网络预测的概率最高的那个数字。这个其实也和误差函数的选择有关。</p><h2><a id="学习与误差" href="#学习与误差" class="headerlink" title="学习与误差"></a>学习与误差</h2><p>神经网络本身确实是一个函数，怎么把这个函数造出来？</p><p>C++ 里写一个函数，我们直接给出它的定义就行，这是因为你脑子里是知道函数的计算过程的，只需将它从数学语言翻译为 C++ 就行。但神经网络不一样。我们不知道手写数字的计算过程（太复杂），就没法直接给出一个满足要求的函数。为此人们开发了一套学习理论。</p><p>假设这个函数是<script type="math/tex">f^{\ast}</script>。以下我们只讨论神经网络的学习。</p><p><script type="math/tex">f^{\ast}</script>是由训练数据来拟合的。也就是说我有<script type="math/tex">n</script>组训练数据<script type="math/tex">(x_i, f^{\ast}(x_i))</script>，要求你写出一个函数，满足<script type="math/tex">\forall i, \;f(x_i) = f^{\ast}(x_i)</script>。当然，实际情况下我们不知道<script type="math/tex">f^{\ast}</script>，所以能看到的就是人工标记的训练数据<script type="math/tex">(x_i, y_i = f^{\ast}(x_i))</script>。</p><p>学习是指调整神经网络，使其输出的向量尽可能服从训练数据的分布（也就是说提高在训练数据集上的准确率）。</p><p>但是无论你怎么学，都是有误差的。</p><ul><li>首先这个网络拟合训练数据分布的程度，对应训练数据集上的准确率。对于这个我们就需要选择一个误差函数，使得误差函数的降低能够提高神经网络的拟合程度（大多数误差函数都是这样的），将我们的目标转化为最小化误差函数。</li><li>其次训练数据的分布拟合<script type="math/tex">f^{\ast}</script>的程度，这个的解决方法是提供更多的数据，或者使用一些数据增强方法。</li></ul><p>综上，神经网络的学习就转化为最小化误差函数。</p><p>有误差，怎么根据误差来调整神经网络？由于误差函数和神经网络上的运算大多是初等函数相关的运算，因此我们可以通过类似牛顿迭代的方式，配合一些随机化的方法，通过求导来更新网络，寻找一个足够优秀的局部最优解。</p><h2><a id="神经网络" href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h2><p>我们先说简单的分层神经网络。用<script type="math/tex">a</script>表示神经网络某一层的计算结果（第<script type="math/tex">0</script>层表示读入）用上指标表示层的编号。一个<script type="math/tex">L</script>层的神经网络中，每一层都对应一个函数<script type="math/tex">f^{(l)}</script>。整个神经网络就是这<script type="math/tex">L</script>个函数的复合。也就是说</p><script type="math/tex;mode=display">\begin{aligned}
y 
& = f(x) \\
& = f^{(L)} (f^{(L - 1)} (\cdots f^{(2)}( f^{(1)}(x) ) \cdots )) \\
& = \left[f^{(L)}\circ f^{(L - 1)} \circ \cdots \circ f^{(2)} f^{(1)}\right](x)
\end{aligned}</script><p>记<script type="math/tex">a^{(0)} = x</script>，<script type="math/tex">a^{(l)} = f^{(l)} \left(a^{(l - 1)} \right)</script>，其中<script type="math/tex">a^{(0)}, \ldots, a^{(L)}</script>均为（高维）向量。</p><p>我们称一个高维向量每一维的长度组成的元组为向量的形状（shape）。那么自然地，你需要保证<script type="math/tex">f^{(l - 1)}</script>的输出的形状与<script type="math/tex">f^{(l)}</script>输入的形状是一样的。</p><p>下面我们列举一些典型的层。下面的域<script type="math/tex">F</script>可以简单视为为<script type="math/tex">\mathbb{R}</script>。</p><h3><a id="线性层" href="#线性层" class="headerlink" title="线性层"></a>线性层</h3><p>首先我们给出一个简洁的定义：一个线性层对应一个函数<script type="math/tex">f: F^n \to F^m</script>，<script type="math/tex">f(x) = wx + b</script>其中<script type="math/tex">n, m</script>是已知常数，<script type="math/tex">w \in M_{m\times n}(F), b\in F^m</script>。</p><p>重复一遍：<script type="math/tex">f: a^{(l - 1)} \mapsto a^{(l)}</script>满足：</p><script type="math/tex;mode=display">a^{(l)}_j = \left( \sum_{i = 0}^{n - 1} w_{ji} a^{(l - 1)}_i \right) + b_j</script><p><script type="math/tex">w, b</script>都是属于这一层的参量，他们的值是无法直接给出的，需要在学习的过程中不断更新。一个线性层的参量规模是所有参量的分量总数，也就是<script type="math/tex">nm + m</script>。</p><h3><a id="激活层" href="#激活层" class="headerlink" title="激活层"></a>激活层</h3><p>有的地方讲解时，喜欢把激活层当成一个激活函数，放在别的层里面。但是这种混杂在一起的做法不利于后面讲反向传播。所以我们把它直接拿出来作为一个层。</p><p>一个激活层对应一个函数<script type="math/tex">f: F^n \to F^n</script>，且对<script type="math/tex">f: a^{(l - 1)} \mapsto a^{(l)}</script>满足</p><script type="math/tex;mode=display">a^{(l)}_i = \sigma (a^{(l - 1)}_i)</script><p>其中<script type="math/tex">\sigma: F \to F</script>被称作激活函数。常用的激活函数有</p><ul><li>Sigmoid：<script type="math/tex">x \mapsto \frac{1}{1 + e^{-x}}</script>；</li><li>ReLU：<script type="math/tex">x \mapsto [x &gt; 0]x</script>；</li><li>Leaky ReLU：<script type="math/tex">x \mapsto [x&gt;0]x + \alpha[x &lt; 0]x</script>。</li></ul><p>这东西不是一个线性变换，所以没法用矩阵来简洁定义。</p><p>激活函数层的参量数取决于<script type="math/tex">\sigma</script>的参量数。在上面的例子里，Sigmoid 和 ReLU 没有参量，而 Leaky ReLU 中的<script type="math/tex">\alpha</script>如果会在学习的过程中变化，那就算一个参量，否则不算参量。</p><p>设置激活层的意义在于使神经网络变得不那么线性，有助于表示更多的计算方式。</p><h3><a id="卷积层" href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h3><p>卷积层对应一个函数<script type="math/tex">f: F^{p\times n\times m} \to F^{q\times n'\times m'}</script>。<script type="math/tex">p</script>和<script type="math/tex">q</script>表示读入的通道数和输出的通道数。另外<script type="math/tex">n', m'</script>不能随意指定。为了准确描述卷积层，我们需要先介绍它的参量：</p><ul><li>核（kernel）：<script type="math/tex">w \in F^{p\times q\times n_k\times m_k}</script>表示<script type="math/tex">p\times q</script>个大小为<script type="math/tex">n_k\times m_k</script>的卷积核。</li><li>偏移（bias，可以没有）：<script type="math/tex">b\in F^{p\times q}</script>，分别表示每个卷积核的偏移量。</li></ul><p>对<script type="math/tex">f: a^{(l - 1)} \mapsto a^{(l)}</script>，我们先考虑简单的情况。</p><p><strong>p=1, q=1</strong></p><p>这时<script type="math/tex">w</script>的形状退化为<script type="math/tex">n_k\times m_k</script>，<script type="math/tex">b</script>退化为一个单独的参量。首先我们给出二维卷积的计算：</p><script type="math/tex;mode=display">z_{i, j} =
\sum_{x = 0}^{n_k - 1}\sum_{y = 0}^{m_k - 1} 
a^{(l - 1)}_{i+x,j+y} w_{x, y}
+ b</script><p>平凡情况下，<script type="math/tex">a^{(l)}_{i, j} =  z_{i, j}</script>，在这种情况下不难算出<script type="math/tex">n' = n - n_k + 1, m' = m - m_k + 1</script>。</p><p>但是<script type="math/tex">z_{i, j}</script>到<script type="math/tex">a^{(l)}_{i, j}</script>过程中可以有猫腻。假设两个维度上的步长（stride）分别是<script type="math/tex">r_x, r_y</script>。一般地，我们有</p><script type="math/tex;mode=display">a^{(l)}_{i, j} = z_{r_xi, r_yj}</script><p>平凡情况即<script type="math/tex">r_x = r_y = 1</script>。可以发现<script type="math/tex">n' = \lceil \frac{n - n_k + 1}{r_x} \rceil, m' = \lceil\frac{m - m_k + 1}{r_y}\rceil</script>。</p><p>设置步长的意义是减小神经网络的规模。</p><p><strong>一般情况</strong></p><p>为了防止巨大多的公式，我们把<script type="math/tex">F^{n\times m}</script>和<script type="math/tex">F^{n_k\times m_k}</script>的步长为<script type="math/tex">(r_x, r_y)</script>的二维卷积（也就是上面那个式子）记作</p><script type="math/tex;mode=display">\begin{aligned}
C: F^{n\times m}\times F^{n_k\times m_k}\times F
&\to F^{n'\times m'} \\
(x, w, b) 
& \mapsto y
\end{aligned}</script><p>这里的<script type="math/tex">x</script>对应上面<script type="math/tex">p=1</script>时的退化的<script type="math/tex">a^{(l - 1)}</script>，<script type="math/tex">y</script>对应<script type="math/tex">q = 1</script>时的退化的<script type="math/tex">a^{(l)}</script>。在一般情况下：</p><script type="math/tex;mode=display">a^{(l)}_t = \sum_{s = 0}^{p - 1} C(a^{(l - 1)}_s, w_{s, t}, b_{s, t})</script><p>卷积层的参量规模为<script type="math/tex">pq(n_km_k + 1)</script>（注意这和<script type="math/tex">n, m</script>无关，也就是说和读入图片的规模无关）。</p><p>与卷积层相关的还有很多，例如池化层，反卷积层等等。除此以外还有一些优化训练的层，比如 dropout 层。</p><h2><a id="反向传播概述" href="#反向传播概述" class="headerlink" title="反向传播概述"></a>反向传播概述</h2><p>在接下来的表述中，<script type="math/tex">f</script>表示神经网络对应的函数，<script type="math/tex">f^{(l)}</script>表示某一层的函数。</p><p>对于一组训练数据<script type="math/tex">(x, y)</script>，神经网络一个基本的学习包含以下几个步骤：</p><ol><li>前向传播：计算<script type="math/tex">\hat{y} = f(x)</script>。</li><li>计算误差：<script type="math/tex">c = C(y, \hat{y})</script>。</li><li>计算梯度：首先是误差梯度<script type="math/tex">\dfrac{\partial c}{\partial \hat{y}}</script>。由于<script type="math/tex">\hat{y} = a^{(L)}</script>，因此最后一层的输出的梯度就是误差梯度。然后我们一层一层倒推，对每个<script type="math/tex">a^{(l)}</script>均求出<script type="math/tex">\dfrac{\partial c}{\partial a^{(l)}}</script>，同时对神经网络的每一层的参量求出关于<script type="math/tex">c</script>的梯度。</li><li>根据梯度更新神经网络的参量。</li></ol><h2><a id="误差函数" href="#误差函数" class="headerlink" title="误差函数"></a>误差函数</h2><p>下面列举一些简单的误差函数。</p><p><strong>交叉熵</strong>（cross entropy）：两个分布（离散分布可以等价于向量）<script type="math/tex">p, q</script>的交叉熵定义为</p><script type="math/tex;mode=display">H(p, q) = - \sum_{n\in\mathcal{N}} p_n \log q_n</script><p>关于交叉熵有<script type="math/tex">H(p, q) = H(p) + D_{KL}(p \mid\mid q)</script>，其中<script type="math/tex">D_{KL}</script>是 KL 散度，定义为</p><script type="math/tex;mode=display">D_{KL}(p \mid\mid q) = \sum_{n\in\mathcal{N}} p_n \log \frac{p_n}{q_n} = H(p) + H(p, q)</script><p>KL 散度可以衡量两个概率分布的差异程度，因此固定<script type="math/tex">p</script>，那么交叉熵也可以。两者都是在<script type="math/tex">p = q</script>时取到最小值。</p><p><strong>均方差</strong>（mean square error）<script type="math/tex">n</script>表示分量个数：</p><script type="math/tex;mode=display">C(p, q) = \frac{1}{n}\sum_{i = 1}^n (p_i - q_i)^2</script><p>这两个函数都可以用于估计神经网络的误差。交叉熵算出来的误差值的梯度会大一些，利于梯度下降。</p><p>另外，要将一个向量的分量转化为概率分布，常用 softmax 函数：</p><script type="math/tex;mode=display">\begin{aligned}
\text{softmax}: x \mapsto y\\
 y_i = \frac{e^{x_i}}{\sum_{j = 0}^{n-1} e^{x_j}}
\end{aligned}</script><h2><a id="反向传播" href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h2><p>偏导数与梯度：偏导数可以粗暴地理解为对多元函数中的某个变元求导数（把其他变元当作系数）。神经网络中，向量对误差的偏导数被称作梯度（gradient）。</p><p>对向量求偏导其实就是对它的每个分量分别求偏导，也就是说对于向量<script type="math/tex">v = \{v_0, \ldots, v_{n - 1}\}</script>，它的梯度定义为</p><script type="math/tex;mode=display">\frac{\partial c}{\partial v} = \left\{
\frac{\partial c}{\partial v_0},
\frac{\partial c}{\partial v_1},
\ldots,
\frac{\partial c}{\partial v_{n-1}}
\right\}</script><p>高维向量同理。</p><p>下面我们来推导一下简单的分层神经网络的反向传播关系式。</p><h3><a id="线性层-1" href="#线性层-1" class="headerlink" title="线性层"></a>线性层</h3><p>前向传播：<script type="math/tex">f: a^{(l - 1)} \mapsto a^{(l)}</script>满足：</p><script type="math/tex;mode=display">a^{(l)}_j = \left( \sum_{i = 0}^{n - 1} w_{ji} a^{(l - 1)}_i \right) + b_j</script><p>已知<script type="math/tex">\dfrac{\partial c}{\partial a^{(l)}}</script>，我们要求<script type="math/tex">\dfrac{\partial c}{\partial a^{(l-1)}}</script>，那么根据上面的公式，结合链式法则，可以推导出</p><script type="math/tex;mode=display">\begin{aligned}
\frac{\partial c}{\partial a^{(l-1)}_i} 
&= \sum_{j = 0}^{m - 1} \frac{\partial a^{(l)}_j}{\partial a^{(l - 1)}_i} 
\frac{\partial c}{\partial a^{(l)}_j} \\
&= \sum_{j = 0}^{m - 1} w_{ji} 
\frac{\partial c}{\partial a^{(l)}_j}
\end{aligned}</script><p>这里用到的一个简单事实是<script type="math/tex">\dfrac{\partial a^{(l)}_j}{\partial a^{(l - 1)}_i} = w_{ij}</script>。对<script type="math/tex">j</script>求和的原因是，改变<script type="math/tex">a^{(l-1)}_i</script>会造成所有的<script type="math/tex">a^{(l)}_j</script>（<script type="math/tex">0 \le j &lt; n</script>）都发生变化，所以<script type="math/tex">a^{(l - 1)}_i</script>的偏导就是它们变化的和除以<script type="math/tex">a^{(l - 1)}_i</script>的变化量。</p><p>然后对于这一层的参量<script type="math/tex">w, b</script>，我们都要求出他们的梯度：</p><script type="math/tex;mode=display">\begin{aligned}
\frac{\partial c}{\partial w_{ji}} 
&= \frac{\partial a^{(l)}_j}{\partial w_{ji}} 
\frac{\partial c}{\partial a^{(l)}_j} \\
&= a^{(l - 1)}_i \frac{\partial c}{\partial a^{(l)}_j}
\end{aligned}</script><p>这里用到的一个简单事实是<script type="math/tex">\dfrac{\partial a^{(l)}_j}{\partial w_{ji}}  = a^{(l - 1)}_i</script>。接下来对<script type="math/tex">b</script>：</p><script type="math/tex;mode=display">\frac{\partial c}{\partial b_j} = \frac{\partial c}{\partial a^{(l)}_j}</script><p>所求的<script type="math/tex">\dfrac{\partial c}{\partial a^{(l-1)}}</script>用于向上一层传递，<script type="math/tex">\dfrac{\partial c}{\partial w}</script>和<script type="math/tex">\dfrac{\partial c}{\partial b}</script>则是这一层的参量对应的梯度。</p><p>细心的朋友还会发现，线性层的反向传播同样可以写成矩阵乘法的形式：</p><script type="math/tex;mode=display">\begin{aligned}
\frac{\partial c}{\partial a^{(l-1)}} 
&= w^{T} \frac{\partial c}{\partial a^{(l)}} \\

\frac{\partial c}{\partial w} 
&= \frac{\partial c}{\partial a^{(l)}} \left(a^{(l - 1)}\right)^{T} \\

\frac{\partial c}{\partial b} 
&= \frac{\partial c}{\partial a^{(l)}}
\end{aligned}</script><h3><a id="激活层-1" href="#激活层-1" class="headerlink" title="激活层"></a>激活层</h3><p>前向传播：<script type="math/tex">f: a^{(l - 1)} \mapsto a^{(l)}</script>满足</p><script type="math/tex;mode=display">a^{(l)}_i = \sigma (a^{(l - 1)}_i)</script><p>已知<script type="math/tex">\dfrac{\partial c}{\partial a^{(l)}}</script>，我们要求<script type="math/tex">\dfrac{\partial c}{\partial a^{(l-1)}}</script>，那么非常显然</p><script type="math/tex;mode=display">\begin{aligned}
\frac{\partial c}{\partial a^{(l-1)}_i} 
&= \frac{\partial a^{(l)}_i}{\partial a^{(l-1)}_i} \frac{\partial c}{\partial a^{(l)}}\\
&= \sigma'\left(a^{(l-1)}_i\right) \frac{\partial c}{\partial a^{(l)}}
\end{aligned}</script><p>激活层通常没有参量，所以不需要给参量求梯度。有的话推一下式子就行。</p><h2><a id="梯度下降" href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><p>通过反向传播，我们就得到了神经网络的所有参量<script type="math/tex">V</script>的梯度。每一层的梯度拼起来就是整个网络的梯度，记作<script type="math/tex">g = \dfrac{\partial c}{\partial V}</script>。</p><p>接下来我们来说怎么梯度下降（gradient descent）来最小化误差函数。</p><p>我们在讨论梯度下降时是对着整个网络的梯度<script type="math/tex">g</script>讨论，但落实到具体的代码实现时其实还是对每一层的参量分别拿它自己的梯度更新（毕竟梯度下降的公式一般是 element-wise 的）。</p><p>朴素的梯度下降是指<script type="math/tex">V \gets V - \alpha g</script>，其中<script type="math/tex">\alpha</script>是学习率，一般<script type="math/tex">0 &lt; \alpha &lt; 1</script>。</p><p>通俗来说，对于一维的情况，如果导数大于<script type="math/tex">0</script>，那就得往回走；否则就继续向前，这样才能走到一个极小值。所以要找到一个极小值那么我们往导数的反方向走就可以了。</p><h3><a id="分批处理数据（batch）" href="#分批处理数据（batch）" class="headerlink" title="分批处理数据（batch）"></a>分批处理数据（batch）</h3><p>为了提高梯度下降的稳定性，我们可以先求出多个数据的梯度<script type="math/tex">g_1, \ldots, g_T</script>，将它们求一个平均值<script type="math/tex">\overline{g}</script>，再用<script type="math/tex">\overline{g}</script>去做梯度下降。这样做的好处是梯度下降的方向会更加“正确”，训练时的波动幅度也会降低。</p><h3><a id="随机梯度下降" href="#随机梯度下降" class="headerlink" title="随机梯度下降"></a>随机梯度下降</h3><p>随机梯度下降（stochastic gradient descent）是指通过将测试数据随机打乱，然后采用朴素的梯度下降公式不断更新网络的参量。打乱数据是一种数据增强的方式。</p><h3><a id="自适应梯度下降" href="#自适应梯度下降" class="headerlink" title="自适应梯度下降"></a>自适应梯度下降</h3><p>自适应梯度下降是一个大的分类，包含一大堆梯度下降的优化算法。这里放一个比较著名的 Adam 算法（没啥好讲的，毕竟我也没研究）：</p><p><img src="../images/adam.webp" alt="adam"></p><h2><a id="代码实现" href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p>困扰大家的除了神经网络本身，恐怕还有 python 和 NumPy 的使用。</p><p>python 本身不算难，有 C++ 基础的同学可以在半天甚至一个小时内基本学会。但是 NumPy 就比较恶心了，这个库的学习成本远高于 python 本身。</p><p>举个例子：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token operator">>></span><span class="token operator">></span> a <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token operator">>></span><span class="token operator">></span> b <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token operator">>></span><span class="token operator">></span> c <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     <span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     <span class="token punctuation">[</span><span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">]</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span> <span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token operator">>></span><span class="token operator">></span> a <span class="token operator">+</span> <span class="token number">3</span> <span class="token comment"># 向量加数字</span>
array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token operator">>></span><span class="token operator">></span> a <span class="token operator">+</span> b <span class="token comment"># 形状相同的两个向量相加</span>
array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token operator">>></span><span class="token operator">></span> a <span class="token operator">+</span> c <span class="token comment"># 形状不同的两个向量相加</span>
array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">1</span><span class="token punctuation">,</span>  <span class="token number">3</span><span class="token punctuation">,</span>  <span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
       <span class="token punctuation">[</span> <span class="token number">4</span><span class="token punctuation">,</span>  <span class="token number">6</span><span class="token punctuation">,</span>  <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
       <span class="token punctuation">[</span> <span class="token number">7</span><span class="token punctuation">,</span>  <span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">11</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token operator">>></span><span class="token operator">></span> b<span class="token punctuation">[</span>a<span class="token punctuation">]</span> <span class="token comment"># ???</span>
array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token operator">>></span><span class="token operator">></span> c<span class="token punctuation">[</span>a<span class="token punctuation">]</span> <span class="token comment"># ???</span>
array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
       <span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
       <span class="token punctuation">[</span><span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>如果你看懂加号 <code class="inline-code">+</code> 的用法了，说明你具有一些基本的举一反三能力。而如果你能看懂中括号 <code class="inline-code">[]</code> 的使用方法，那么相信 NumPy 对你来说非常容易上手。</p><h3><a id="NumPy-入门" href="#NumPy-入门" class="headerlink" title="NumPy 入门"></a>NumPy 入门</h3><p>NumPy 提供了向量的一系列运算，非常适合数据处理。</p><p><code class="inline-code">np.zeros</code> 可以创建全为<script type="math/tex">0</script>的向量：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> a <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># 创建一个 3 x 2 x 1 的三维向量。内层括号是 tuple 的括号，不能省</span>
<span class="token operator">>></span><span class="token operator">></span> a
array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">0.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>

       <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">0.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>

       <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">0.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token operator">>></span><span class="token operator">></span> a<span class="token punctuation">.</span>shape <span class="token comment"># a 的形状</span>
<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><code class="inline-code">np.random.normal</code> 可以创建一个服从正态分布的随机向量。</p><p>当然，如同上面的示例，<code class="inline-code">np.array</code> 也可以用于创建向量。</p><p>要理解 NumPy 中各种各样函数和运算符的用法，我们需要理解它的一个概念：广播（broadcasting）。</p><h4><a id="Broadcasting1" href="#Broadcasting1" class="headerlink" title="Broadcasting1"></a>Broadcasting<sup><a id="reffn_1" class="headerlink" title="1"></a><a href="#fn_1">1</a></sup></h4><p>NumPy 中大多数的运算是“点对点”计算的（element-wise）。也就是说如果两个向量<script type="math/tex">a, b</script>形状相同，那么<script type="math/tex">a + b, a - b, a\times b, a \div b</script>都是点对点的数值运算：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> a <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token operator">>></span><span class="token operator">></span> b <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token operator">>></span><span class="token operator">></span> a <span class="token operator">+</span> b
array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token operator">>></span><span class="token operator">></span> a <span class="token operator">-</span> b
array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token operator">>></span><span class="token operator">></span> a <span class="token operator">*</span> b
array<span class="token punctuation">(</span><span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">,</span>  <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">12</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token operator">>></span><span class="token operator">></span> a <span class="token operator">/</span> b
array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.</span>        <span class="token punctuation">,</span> <span class="token number">0.2</span>       <span class="token punctuation">,</span> <span class="token number">0.33333333</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token operator">>></span><span class="token operator">></span> <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>但是当两个向量形状不同时，NumPy 会按照一定的 broadcast 规则将小的向量补成大的向量，然后执行 element-wise 的计算（这个是对于常规的函数，有的函数比如点积 dot 的 broadcast 规则是不同的）。</p><p>具体来说，假设<script type="math/tex">a</script>的形状是<script type="math/tex">(s_1, \ldots, s_n)</script>，<script type="math/tex">b</script>的形状是<script type="math/tex">(t_1, \ldots, t_m)</script>。不妨<script type="math/tex">n &gt; m</script>。</p><p>首先在<script type="math/tex">b</script>的开头补<script type="math/tex">1</script>，把它从<script type="math/tex">m</script>维变成<script type="math/tex">n</script>维向量。因此我们不妨设<script type="math/tex">b</script>的形状是<script type="math/tex">(t_1, \ldots, t_n)</script>。</p><p>然后我们按照<script type="math/tex">i = n, n - 1, \ldots, 2, 1</script>的顺序从最后一维倒着开始依次比较。如果<script type="math/tex">s_i = t_i</script>或者<script type="math/tex">s_i = 1</script>或者<script type="math/tex">t_i = 1</script>，那么我们认为这两个维度兼容，否则就不兼容。</p><p>如果存在不兼容的维度，那么 NumPy 会直接报错，中断程序。</p><p>如果<script type="math/tex">s_i = 1 &lt; t_i</script>，那就把<script type="math/tex">s_i</script>补成<script type="math/tex">t_i</script>，相应下标的数值直接复制过来，即<script type="math/tex">a_{j_1,j_2, \ldots, j_n} = a_{j_1,j_2, \ldots, j'_i = 1, \ldots, j_n}</script>。<script type="math/tex">t_i = 1 &lt; s_i</script>同理。</p><p>操作完之后，两个数组的形状就相同了，然后就正常做计算就可以了。</p><h4><a id="Indexing2" href="#Indexing2" class="headerlink" title="Indexing2"></a>Indexing<sup><a id="reffn_2" class="headerlink" title="2"></a><a href="#fn_2">2</a></sup></h4><p>在 C++ 中，<code class="inline-code">a[i]</code> 表示获取数组 <code class="inline-code">a</code> 下标为 <code class="inline-code">i</code> 的元素。NumPy 对此有一些狂野的用法。</p><p>假设<script type="math/tex">a</script>的形状是<script type="math/tex">(s_1, \ldots, s_n)</script>，那么 <code class="inline-code">a[x1, x2, ..., xn]</code> 表示获取<script type="math/tex">a_{x1, \ldots, x_n}</script>的值。</p><p><code class="inline-code">a[[r1, ..., rs]]</code> 表示将第<script type="math/tex">r_1, \ldots, r_s</script>行取出来组成一个新的高维向量：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> c
array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
       <span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
       <span class="token punctuation">[</span><span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token operator">>></span><span class="token operator">></span> c<span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token comment"># 取出第 0 行和第 2 行</span>
array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
       <span class="token punctuation">[</span><span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>有了这两个基本常识后，再去理解 NumPy 的函数就会容易很多。</p><h3><a id="指南" href="#指南" class="headerlink" title="指南"></a>指南</h3><p>接下来我们给出一些 NumPy 实现手写数字识别时会用到的实现，帮大家少走一些弯路。</p><ul><li><p>首先我们在实现的时候，向量一般是一维，也就是行向量，而不是<script type="math/tex">n\times 1</script>的列向量，因此上面的前向传播和反向传播公式其实得转置。</p></li><li><p>矩阵乘法调用 <code class="inline-code">np.matmul</code> 即可，具体用法查看文档。</p></li><li><p><code class="inline-code">a.reshape</code> 可以在总的元素数不变的情况下调整向量的形状，例如 <code class="inline-code">a.reshape((n, 1))</code> 可以将 <code class="inline-code">a</code> 变成<script type="math/tex">n\times 1</script>的二维数组（前提是 <code class="inline-code">a</code> 的元素数为<script type="math/tex">n</script>，否则会报错）。</p></li><li><p><code class="inline-code">np.transpose</code> 和 <code class="inline-code">a.T</code> 都可以实现矩阵的转置。</p></li><li><p>ReLU 函数和 ReLU 的导数分别可以写成</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 这里的 x 的类型是 NumPy 的数组</span>

<span class="token keyword">def</span> <span class="token function">ReLU</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
	<span class="token keyword">return</span> <span class="token punctuation">(</span>x <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token operator">*</span> x

<span class="token keyword">def</span> <span class="token function">ReLU_prime</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> <span class="token punctuation">(</span>x <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token number">1</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p>softmax 函数当<script type="math/tex">x_i</script>较大时，<script type="math/tex">e^{x_i}</script>会爆精度。因此需要使用一些计算技巧。注意到将<script type="math/tex">x</script>所有分量减掉同一个数，softmax 不变，因此我们可以将他们都减掉最大值。而 <code class="inline-code">np.sum</code> 可以求出一个高维向量的所有分量的和。于是 softmax 函数可以写成</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">softmax</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    x <span class="token operator">=</span> np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>x <span class="token operator">-</span> x<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> x <span class="token operator">/</span> x<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li><li><p>交叉熵函数可以写成</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">crossentropy</span><span class="token punctuation">(</span>p<span class="token punctuation">,</span> q<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment"># p, q 长度相同</span>
    <span class="token keyword">return</span> <span class="token operator">-</span><span class="token punctuation">(</span>p <span class="token operator">*</span> np<span class="token punctuation">.</span>log<span class="token punctuation">(</span>q<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># mean 表示平均值，log 底数默认为 e</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li><li><p><code class="inline-code">np.eye(n)</code> 可以创建<script type="math/tex">n</script>阶单位矩阵；<code class="inline-code">np.tile</code> 可以将一个向量复制若干次。</p></li><li><p>如果基础的 NumPy 操作你已经熟练，想要优化一个 batch 的反向传播的复杂度，在计算线性层的<script type="math/tex">\dfrac{\partial c}{\partial w}</script>时会遇到这样的式子：</p><script type="math/tex;mode=display">\frac{\partial c}{\partial w_{ji}}
= \sum_{t=0}^{T} a^{(l - 1)}_{t,i} \left(\frac{\partial c}{\partial a^{(l)}}\right)_{t, j}
= \sum_{t=0}^{T} A_{t,i} B_{t, j}</script><p>这种情况可以使用 <code class="inline-code">np.tensordot</code> <sup><a id="reffn_3" class="headerlink" title="3"></a><a href="#fn_3">3</a></sup>函数：<code class="inline-code">np.tensordot(B, A, ([0, 0]))</code>，表示将两者的第<script type="math/tex">1</script>维一起枚举并求和，其他维按顺序依次排下来。</p></li><li><p>其他常用的函数有 <code class="inline-code">np.atleast_2d, np.flatten, np.concatenate</code>。</p></li></ul><h2><a id="Norms" href="#Norms" class="headerlink" title="Norms"></a>Norms</h2><p><img src="../images/brief_nn_norm.png" alt="brief_nn_norm"></p><p>总体的计算公式为</p><script type="math/tex;mode=display">y = \frac{x - E[x]}{\sqrt{Var[x] + \varepsilon}} \ast \gamma + \beta</script><p><a id="fn_1" class="headerlink" title="1"></a><sup>1</sup>. <a target="_blank" rel="noopener" href="https://numpy.org/doc/stable/user/basics.broadcasting.html">https://numpy.org/doc/stable/user/basics.broadcasting.html</a><a href="#reffn_1" title="Jump back to footnote [1] in the text."> &#8617;</a></p><p><a id="fn_2" class="headerlink" title="2"></a><sup>2</sup>. <a target="_blank" rel="noopener" href="https://numpy.org/doc/stable/user/basics.indexing.html">https://numpy.org/doc/stable/user/basics.indexing.html</a><a href="#reffn_2" title="Jump back to footnote [2] in the text."> &#8617;</a></p><p><a id="fn_3" class="headerlink" title="3"></a><sup>3</sup>. <a target="_blank" rel="noopener" href="https://numpy.org/doc/stable/reference/generated/numpy.tensordot.html#numpy-tensordot">https://numpy.org/doc/stable/reference/generated/numpy.tensordot.html#numpy-tensordot</a><a href="#reffn_3" title="Jump back to footnote [3] in the text."> &#8617;</a></p></div><hr><h3><span class="iconfont icon-history" style="padding-right:5px;font-size:1em"></span>修订记录</h3><div class="post-history-list"><ol reversed><li class="post-history-item">2023年11月25日 第3次修订</li><li class="post-history-item">2023年4月29日 第2次修订</li><li class="post-history-item">2023年4月16日 创建文章</li></ol></div><div class="post-footer"><hr><div class="next-post"><span class="iconfont icon-arrow-left-circle"></span> <a href="/Math/Reflection-Inc-Exc/">反射容斥学习笔记</a></div><div class="prev-post"><a href="/Math/Burnside-and-Polya/">Burnside 引理入门 </a><span class="iconfont icon-arrow-right-circle"></span></div></div></article></main></body><div class="side-button"><div id="comment-button" class="button" title="Valine Comment"><span class="iconfont icon-comment"></span></div><div id="darkmode-button" class="button" title="Switch between day and night"><span class="iconfont icon-moonbyueliang"></span></div><div id="top" class="button" title="Back to top"><span class="iconfont icon-arrowup"></span></div></div><footer id="footer" style="margin:3em 0 2em 0;text-align:center;line-height:1.4em"><span class="post-wordcount iconfont icon-text"></span> 全站约 404,069 字<div class="license" style="padding:0 10px"><span>本站所有文章遵循许可协议 <a target="_blank" rel="license noreferrer noopener" href="https://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a></span></div><div class="credit"><span>Themed <a href="https://github.com/sshwy/hexo-theme-essence" target="_blank" rel="noreferrer noopener">Essence v1.9.11</a> | Powered by <a href="http://hexo.io" target="_blank" rel="noreferrer noopener">Hexo</a></span></div><div class="friends"><span class="friend-link-span">友情链接 </span><span class="icon friendsep"></span> <a target="_blank" rel="noopener" href="https://carol-xrl.github.io/"><span class="friend-link-span">Carol </span></a><span class="icon friendsep"></span> <a target="_blank" rel="noopener" href="https://www.cnblogs.com/GavinZheng"><span class="friend-link-span">GavinZheng </span></a><span class="icon friendsep"></span> <a target="_blank" rel="noopener" href="https://www.cnblogs.com/xryjr233"><span class="friend-link-span">xryjr233 </span></a><span class="icon friendsep"></span> <a target="_blank" rel="noopener" href="https://www.cnblogs.com/dysyn1314"><span class="friend-link-span" title="为什么我的头常常变绿？因为…… "><span style="font-weight:700"><span class="codeforces p">dy</span><span class="codeforces m">syn</span><span class="codeforces p">1314</span></span> </span></a><span class="icon friendsep"></span> <a target="_blank" rel="noopener" href="https://blog.aor.sd.cn"><span class="friend-link-span">RainAir </span></a><span class="icon friendsep"></span> <a target="_blank" rel="noopener" href="https://www.cnblogs.com/lcyfrog"><span class="friend-link-span">lcyfrog </span></a><span class="icon friendsep"></span> <a target="_blank" rel="noopener" href="https://www.cnblogs.com/maoyiting"><span class="friend-link-span">/hanx/se</span></a></div><div class="copyright"><span>Copyright &copy; 2019-present - Sshwy</span></div></footer><script src="/js/ssimple.js?20250423.js"></script><script src="https://pv.sohu.com/cityjson?ie=utf-8.js" defer></script></html>