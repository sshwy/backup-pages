<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=0" name="viewport"><meta name="keywords" content=""><meta name="description" content="Sshwy 的个人博客"><meta name="generator" content="Hexo 6.3.0"><title>随机算法总结 - Sshwy&#39;s Notes</title><link rel="shortcut icon" href="/images/favicon.ico"><link rel="stylesheet" href="/css/ssimple.css?20250423.css"><link rel="stylesheet" href="/iconfont/iconfont.css?20250423.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/katex.min.css"><script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/katex.min.js" crossorigin="anonymous"></script><script>document.addEventListener("DOMContentLoaded",function(){window.renderKatex()})</script></head><body><header><div class="header" style="width:100%"><div class="header-inner" style="position:fixed;z-index:20;top:0;left:0"><div class="header-content" style="max-width:1200px;margin:auto"><div class="blog-title"><span class="iconfont icon-menu1" id="menu-button"></span> <a href="/" class="logo header-title">Sshwy&#39;s Notes</a><div class="mobile-search"><input type="text"> <span class="iconfont icon-search mobile-search-icon"></span> <span class="iconfont icon-baseline-close-px search-close-icon"></span></div></div><div class="navbar" id="menu-list"><ul class="menu"><li class="menu-item"><a href="/archives/" class="menu-item-link"><span class="menu-item-icon iconfont icon-work"></span> Archives</a></li><li class="menu-item"><a href="/directory/" class="menu-item-link"><span class="menu-item-icon iconfont icon-folder-close"></span> Directory</a></li><li class="menu-item"><a href="/about/" class="menu-item-link"><span class="menu-item-icon iconfont icon-user"></span> About</a></li><li class="menu-item"><a href="/static/beibishi2023" class="menu-item-link"><span class="menu-item-icon iconfont icon-favorite"></span> NOI 背笔试</a></li></ul></div><div class="search"><input type="text"> <span class="iconfont icon-search search-icon"></span> <span class="iconfont icon-baseline-close-px search-close-icon"></span></div></div></div><div class="search-shadow"></div><div class="search-box"><div class="search-container"><div class="search-container-inner"><div class="search-data-status"><span>Fetching search data...</span></div><div class="search-count"></div><div class="search-result"></div></div></div></div></div></header><main class="main"><article class="post"><div class="post-title"><h1 class="page-title">随机算法总结</h1></div><div class="post-meta"><div class="post-info"><span class="post-info-item post-time" title="2023年6月23日星期五上午11点12分 (CST+08:00)"><span class="info-icon iconfont icon-time"></span>更新于 2023年6月23日 </span><span class="icon infosep"></span><span class="post-info-item post-wordcount"> <span class="info-icon iconfont icon-text"></span>约 5,802 字</span></div><div class="post-directory"><span class="iconfont icon-folder-close"></span> <a class="directory" href="/directory">Home</a> <a class="directory" href="/directory/"></a> <span class="icon smallarrow"></span> <a class="directory" href="/directory/Randomized/">随机算法专题 </a><span class="icon smallarrow"></span> 当前文章</div><div class="post-tags"><span class="tag"><span class="meta-icon iconfont icon-tag"></span><a class="tag" href="/tags/Notes/">Notes</a></span></div><hr></div><div class="toc"><h2 class="toc-title"><span class="iconfont icon-explain" style="font-size:1em;padding-right:5px"></span>文章目录</h2><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Count-min-Sketch"><span class="toc-number">1.</span> <span class="toc-text">Count-min Sketch</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Karp-Rabin-Hash"><span class="toc-number">2.</span> <span class="toc-text">Karp-Rabin Hash</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Min-Hash"><span class="toc-number">3.</span> <span class="toc-text">Min-Hash</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Sim-Hash"><span class="toc-number">4.</span> <span class="toc-text">Sim-Hash</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Hamming-NN-Approximation"><span class="toc-number">5.</span> <span class="toc-text">Hamming-NN Approximation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Well-Separated-Pair-Decomposition"><span class="toc-number">6.</span> <span class="toc-text">Well-Separated Pair Decomposition</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#t-Spanner"><span class="toc-number">7.</span> <span class="toc-text">t-Spanner</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Tree-Embedding"><span class="toc-number">8.</span> <span class="toc-text">Tree Embedding</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Johnson-Lindenstrauss-Transform"><span class="toc-number">9.</span> <span class="toc-text">Johnson-Lindenstrauss Transform</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Power-Iteration"><span class="toc-number">10.</span> <span class="toc-text">Power Iteration</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Sparse-Recovery"><span class="toc-number">11.</span> <span class="toc-text">Sparse Recovery</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Stream’s-Sparse-Recovery"><span class="toc-number">11.1.</span> <span class="toc-text">Stream’s Sparse Recovery</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Compressed-Sensing"><span class="toc-number">11.2.</span> <span class="toc-text">Compressed Sensing</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ell-0-Sampling"><span class="toc-number">12.</span> <span class="toc-text">\ell_0-Sampling</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Stream-MST-Approximation"><span class="toc-number">12.1.</span> <span class="toc-text">Stream MST Approximation</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Comments-amp-References"><span class="toc-number">13.</span> <span class="toc-text">Comments &amp; References</span></a></li></ol></div><hr></div><div class="post-content"><h2><a id="Count-min-Sketch" href="#Count-min-Sketch" class="headerlink" title="Count-min Sketch"></a>Count-min Sketch</h2><p>这是一种，估计出现次数的数据结构，适用于数据流（强在线），不同的元素数不多的情况。</p><p>基本思想是一个哈希表可以估计某个值的出现的次数，但是会估多，因此多个哈希取最小值即可。这里的哈希要取均匀哈希才能保证理论复杂度的正确性。</p><p>详见 <a href="/Randomized/Count-min-Sketch/">「随机算法专题」Count-min Sketch 入门</a>。</p><h2><a id="Karp-Rabin-Hash" href="#Karp-Rabin-Hash" class="headerlink" title="Karp-Rabin Hash"></a>Karp-Rabin Hash</h2><p>简单的字符串哈希：<script type="math/tex">h(s) \triangleq \sum_{i = 1}^{|s|} s_ix^{|s| - i}</script>。区间哈希：</p><script type="math/tex;mode=display">h(s[l, r]) = h(s[1, r]) - h(s[1, l-1])x^{r-l+1}</script><p>二维的情况：<script type="math/tex">h(a_{n\times m})\triangleq \sum_{i = 1}^{n}\sum_{j = 1}^m a_{ij}x^{n - i}y^{n - j}</script>。矩阵哈希（差分）：</p><script type="math/tex;mode=display">h(a[l_1, r_1][l_2, r_2]) = [h(a[1, r_1][1, r_2]) - h(a[1, r_1][1, l_2 - 1])y^{r_2 - l_2 + 1}]\\
-[h(a[1, l_1-1][1, r_2]) - h(a[1, l_1-1][1, l_2 - 1])y^{r_2 - l_2 + 1}]x^{r_1 - l_1 + 1}</script><h2><a id="Min-Hash" href="#Min-Hash" class="headerlink" title="Min-Hash"></a>Min-Hash</h2><p>估计两个集合（这里考虑整数集）的相似度，可以使用交并比来描述：<script type="math/tex">J(A, B) = \frac{|A\cap B|}{|A\cup B|}</script><sup><a id="reffn_1" class="headerlink" title="1"></a><a href="#fn_1">1</a></sup>。</p><p>同样是考虑一个数据流的背景，每个集合只能遍历一次（预处理），然后我们若干次询问两个集合的相似度。</p><p>考虑一个均匀随机哈希<script type="math/tex">h</script>。那么<script type="math/tex">J(A, B) = J(h(A), h(B)) = E_x[x\in h(A)\cup h(B)]</script>。我们的大致思路是构造<script type="math/tex">T</script>个不同的哈希<script type="math/tex">h</script>，每次随机一个<script type="math/tex">x</script>，判断是否<script type="math/tex">x\in h(A) \cup h(B)</script>，然后取平均值。</p><p>考虑每次随机的<script type="math/tex">x = \min h(A \cup B)</script>。显然<script type="math/tex">x\in  h(A) \cup h(B)\iff x = \min h(A) = \min h(B)</script>。这样的<script type="math/tex">x</script>由于每个哈希函数<script type="math/tex">h</script>不同的随机性，可以看成是随机的。</p><h2><a id="Sim-Hash" href="#Sim-Hash" class="headerlink" title="Sim-Hash"></a>Sim-Hash</h2><p>估计两个高维向量的相似度，可以考虑估计他们的夹角<script type="math/tex">\theta(x, y)</script><sup><a id="reffn_2" class="headerlink" title="2"></a><a href="#fn_2">2</a></sup>。</p><p>考虑一个高维随机高斯向量（每一位高斯分布，然后单位化）<script type="math/tex">w</script>，那么<script type="math/tex">E[\mathbf{1}_{(x, w)\ge 0} \oplus \mathbf{1}_{(y, w)\ge 0}] = \frac{\theta(x, y)}{\pi}</script>。</p><p>设<script type="math/tex">h(x, w) = \mathbf{1}_{(x, w)\ge 0}</script>，取<script type="math/tex">T</script>个随机的高斯向量<script type="math/tex">w_1, \ldots, w_T</script>，那么可以视<script type="math/tex">x\mapsto \{h(x, w_i)\}</script>为一个”保度量“的<script type="math/tex">\mathbf{R}^n\to \mathbf{H}^T</script>变换<sup><a id="reffn_3" class="headerlink" title="3"></a><a href="#fn_3">3</a></sup>。</p><h2><a id="Hamming-NN-Approximation" href="#Hamming-NN-Approximation" class="headerlink" title="Hamming-NN Approximation"></a>Hamming-NN Approximation</h2><p>在<script type="math/tex">\mathbf{H}^d</script>（<script type="math/tex">d</script>约为<script type="math/tex">64</script>左右）中给定<script type="math/tex">p_1, \ldots, p_n</script>，每次询问<script type="math/tex">q</script>，查询<script type="math/tex">p_i</script>到<script type="math/tex">q</script>的最近邻。</p><p>Hamming 空间的度量是特殊的，并且每个维度只有<script type="math/tex">0</script>或<script type="math/tex">1</script>。因此距离的分析可以转化为不同分量个数的分析。</p><p>考虑把这件事变得随机一点。我们把每个点（<script type="math/tex">q</script>和<script type="math/tex">p_i</script>）作用一个排列<script type="math/tex">\sigma</script>来均匀随机打乱坐标。然后我们在<script type="math/tex">\{ \sigma p_i \}</script>中找与<script type="math/tex">\sigma q</script>的 LCP 最大的<script type="math/tex">N</script>个点加入候选集合<script type="math/tex">S</script>。多次取不同的排列<script type="math/tex">\sigma</script>，最后求<script type="math/tex">S</script>到<script type="math/tex">q</script>的最近邻即可<sup><a id="reffn_4" class="headerlink" title="4"></a><a href="#fn_4">4</a></sup>。</p><p>总体思路类似于将 Hamming 距离通过重排列来近似转化为 LCP 长度。</p><h2><a id="Well-Separated-Pair-Decomposition" href="#Well-Separated-Pair-Decomposition" class="headerlink" title="Well-Separated Pair Decomposition"></a>Well-Separated Pair Decomposition</h2><p>对点集<script type="math/tex">P</script>的一个<script type="math/tex">\varepsilon</script>-WSPD 是一系列欧氏空间点集对<script type="math/tex">\{ (A_i, B_i) \}</script>满足</p><ol><li><script type="math/tex">\max(\operatorname{diam} A_i, \operatorname{diam} B_i) \le \varepsilon \operatorname{dist}(A_i, B_i)</script>（点集的距离为点对距离的最小值）；</li><li><script type="math/tex">\bigcup A_i\times B_i = P\times P</script>。</li></ol><p>基于任意递归的空间划分形成的树形结构都可以有一个朴素的<script type="math/tex">\varepsilon</script>-WSPD 构造方法：</p><ol><li>初始的点集对为<script type="math/tex">(P, P)</script>；</li><li>对于当前的点集对<script type="math/tex">(A, B)</script>，不妨<script type="math/tex">\operatorname{diam}A &gt; \operatorname{diam}B</script>。如果<script type="math/tex">(A, B)</script>满足条件就直接返回，否则将<script type="math/tex">A</script>按照树形结构的划分拆分成子集<script type="math/tex">\{A_i\}</script>，然后对<script type="math/tex">(A_i, B)</script>递归地构造 WSPD 即可。</li></ol><p>复杂度<script type="math/tex">O(n\varepsilon^{-d}\log\Delta)</script>，<script type="math/tex">\Delta</script>为值域，<script type="math/tex">d</script>为空间维度。二维平面的情况可以四分树或者 KD-Tree，后者事实上任意维度均通用。</p><p>构造<script type="math/tex">\varepsilon = \frac{1}{2}</script>的 WSPD，则最近点对必然是某个孤点集对（但其实求最近点对可以直接 KD-Tree）。</p><h2><a id="t-Spanner" href="#t-Spanner" class="headerlink" title="t-Spanner"></a>t-Spanner</h2><p><script type="math/tex">t</script>-Spanner<sup><a id="reffn_7" class="headerlink" title="7"></a><a href="#fn_7">7</a></sup> 是欧氏空间点集<script type="math/tex">P</script>的一个欧氏子图<script type="math/tex">H = (P, E)</script>，其中<script type="math/tex">E\subseteq P\times P</script>。<script type="math/tex">H</script>上的距离定义为最短路，边长为欧氏距离。并且<script type="math/tex">\forall x, y\in P</script>有<script type="math/tex">\Vert x - y \Vert_2 \le \operatorname{dist}_H(x, y) \le t\Vert x - y \Vert_2</script>。</p><p>可以用 WSPD 来构造<script type="math/tex">(1 + \varepsilon)</script>-Spanner：构造<script type="math/tex">\frac{\varepsilon}{4}</script>-WSPD，然后将代表点连边（总共<script type="math/tex">O(n\varepsilon^{-d}\log \Delta)</script>条边）。</p><p>证明可以通过对<script type="math/tex">\Vert x - y \Vert_2</script>归纳来完成。</p><p>在<script type="math/tex">(1 + \varepsilon)</script>-Spanner 上求 MST 可以得到点集<script type="math/tex">P</script>的一个<script type="math/tex">(1 + \varepsilon)</script>-近似 MST。</p><h2><a id="Tree-Embedding" href="#Tree-Embedding" class="headerlink" title="Tree Embedding"></a>Tree Embedding</h2><p>类似 Spanner，Tree Embedding 是将<script type="math/tex">d</script>维欧氏空间<script type="math/tex">[0, \Delta]^d</script>上的点集<script type="math/tex">P</script>映射到一棵树<script type="math/tex">T = (V, E)</script>（<script type="math/tex">P\subseteq V</script>），将点对的距离用树上的最短路距离代替。衡量指标为 distortion：<script type="math/tex">\max_{x\ne y} \frac{\operatorname{dist}_T(x, y)}{\Vert x - y\Vert_2}</script>，越小越好。</p><p>考虑构造一个<script type="math/tex">2^d</script>分树，然后将其随机平移（等价于固定<script type="math/tex">2^d</script>分树的总体范围，然后平移整个平面）。数据点构成树的叶子，<strong>高度</strong>为<script type="math/tex">i</script>的点到<strong>高度</strong>为<script type="math/tex">i + 1</script>的点的边权值为<script type="math/tex">\sqrt{d}2^i</script>（假设所有叶子结点深度相同）。<script type="math/tex">\sqrt{d}</script>可以理解为<script type="math/tex">d</script>维立方体对角线长度，是一个距离的常数。</p><p>这个做法的 distortion 是期望<script type="math/tex">O(d h)</script>级别。<script type="math/tex">h</script>是<script type="math/tex">2^d</script>分树的高度。</p><p>也可以用 KD-Tree 做一个魔改版，但是会导致逼近效果减弱（相当于曼哈顿距离）。</p><h2><a id="Johnson-Lindenstrauss-Transform" href="#Johnson-Lindenstrauss-Transform" class="headerlink" title="Johnson-Lindenstrauss Transform"></a>Johnson-Lindenstrauss Transform</h2><p>Johnson-Lindenstrauss 是一种降维变换<script type="math/tex">f: \mathbf{R}^d \to \mathbf{R}^m</script>，能够一定误差内保<script type="math/tex">n</script>个<script type="math/tex">\mathbf{R}^d</script>上的点<script type="math/tex">p_i</script>两两之间的距离：<script type="math/tex">\Vert f(p_i) - f(p_j) \Vert_2 \in (1\pm \varepsilon) \Vert p_i - p_j \Vert_2</script><sup><a id="reffn_5" class="headerlink" title="5"></a><a href="#fn_5">5</a></sup>。</p><p>应用：快速 Linear Regression。其本质是求<script type="math/tex">\arg\min_w \Vert Xw - y \Vert_2</script>（<script type="math/tex">X\in \mathbf{R}^{n\times d}, y\in \mathbf R^n, w\in \mathbf R^d</script>）。这里<script type="math/tex">X</script>事实上可以看成<script type="math/tex">d</script>个<script type="math/tex">n</script>维空间上的点（转置），于是线性回归就变成了一个欧氏距离下的近似线性表出的问题。</p><p>Subspace 版本 JL：</p><ul><li>原始方法：构造<script type="math/tex">A = \frac{1}{\sqrt m} (w_1, \ldots, w_m)^T</script>，<script type="math/tex">w_i\in \mathbf R^n</script>为随机高斯向量，<script type="math/tex">A\in \mathbf R^{m\times n}</script>。</li><li>更优的做法：生成每列（随机）只有一个非零元素的 01 矩阵<script type="math/tex">A_{m\times n}</script>。</li></ul><p>问题转化为<script type="math/tex">\arg\min_w \Vert AXw-Ay\Vert_2</script>。设<script type="math/tex">\tilde X = AX, \tilde y = Ay</script>，取<script type="math/tex">m = d</script>，则<script type="math/tex">\tilde X^T\tilde X</script>大概率可逆。于是<script type="math/tex">\hat w \approx (\tilde X^T\tilde X)^{-1} \tilde X^T y</script><sup><a id="reffn_6" class="headerlink" title="6"></a><a href="#fn_6">6</a></sup>。</p><h2><a id="Power-Iteration" href="#Power-Iteration" class="headerlink" title="Power Iteration"></a>Power Iteration</h2><p>主成分分析（Principal Component Analysis）是另外一种降维方法，主旨是对于<script type="math/tex">n</script>个<script type="math/tex">d</script>维空间上的点<script type="math/tex">p_i</script>找到<script type="math/tex">m</script>个<strong>单位正交</strong>向量<script type="math/tex">v_1, \ldots, v_m</script>，最大化<script type="math/tex">\frac{1}{n} \sum_{i = 1}^n\sum_{j = 1}^m (x_i, v_j)^2</script>（在此子空间上的投影长度和）。</p><p>Power Iteration 一种粗暴的求最大主成分（Top Principal Component）的算法。</p><p>不妨设<script type="math/tex">X = (x_1, \ldots, x_n)^T \in \mathbf R^{n\times d}</script>，那么 TPC 的目标可以写成<script type="math/tex">\max \Vert Xv\Vert _2 = \max v^TX^TXv</script>。</p><p>设<script type="math/tex">A = X^T X</script>是实对称矩阵，可以对角化。设<script type="math/tex">A = Q^TDQ</script>，并不妨设<script type="math/tex">D_{1,1}</script>是最大的特征值。则目标可以写成<script type="math/tex">\max (Qv)^T DQv</script>，结论是<script type="math/tex">\hat v</script>取<script type="math/tex">Q^T</script>第一列时最大化（因为算出来就是<script type="math/tex">D_{1,1}</script>）。</p><p>注意到<script type="math/tex">A = Q^T D Q</script>的本质是先旋转，再拉伸，再旋转（单位球映射成椭球），拉伸的方向就对应了 TPC 的方向。因此我们对任意一个向量<script type="math/tex">u</script>，只要其不垂直于<script type="math/tex">\hat v</script>，就可以在反复应用<script type="math/tex">A</script>后拉伸到 TPC 的方向。</p><p>因此我们不断计算<script type="math/tex">A^k u</script>然后单位化即可。</p><p>另外如果<script type="math/tex">X</script>足够稀疏，那么可以用<script type="math/tex">Au = X^T(Xu)</script>的方式计算。</p><h2><a id="Sparse-Recovery" href="#Sparse-Recovery" class="headerlink" title="Sparse Recovery"></a>Sparse Recovery</h2><p>数据流（Streaming）模型：可以描述为一个插入/删除的操作构成的序列，只允许单次、顺序访问，在结束后回答一些询问，一般不要求实时，但是会有较强的空间限制。</p><p>前面的 Min-Hash，Sim-Hash，Hamming-NN 均可看作数据流算法。</p><p>事实上数据集可以等价地用每个元素出现的频数构成的频数向量<script type="math/tex">x</script>表达<sup><a id="reffn_8" class="headerlink" title="8"></a><a href="#fn_8">8</a></sup>。</p><p>记<script type="math/tex">\operatorname{supp}(x) = \{ p \mid x_p \ne 0\}</script>表示数据流中出现过的所有元素。定义<script type="math/tex">\ell_0</script>范数：<script type="math/tex">\Vert x\Vert _0 = |\operatorname{supp}(x)|</script>。若<script type="math/tex">\Vert x\Vert_0 \le k</script>，称之<script type="math/tex">k</script>-sparse。</p><p>Sparse Recovery 是一个数据流算法，能够给定<script type="math/tex">k</script>：</p><ol><li>检测数据流是否<script type="math/tex">k</script>-sparse；</li><li>将<script type="math/tex">k</script>-sparse 的数据流的<script type="math/tex">\operatorname{supp}(x)</script>完全恢复</li></ol><p><strong>直径估计</strong>：设<script type="math/tex">\mathcal S_i</script>为<script type="math/tex">2^i\varepsilon</script>边长的格点划分，<script type="math/tex">i = 0, 1, \ldots, O(\log \Delta)</script>。在数据流的过程中对一个点的增删转化为其在<script type="math/tex">\mathcal S_i</script>所在格子的代表点的增删。询问时，找到最小的<script type="math/tex">i_{\min}</script>使得<script type="math/tex">\mathcal S_{i_{\min}}</script>是<script type="math/tex">k</script>-sparse 的，然后在<script type="math/tex">\mathcal S_{i_{\min}}</script>上估计直径。</p><h3><a id="Stream’s-Sparse-Recovery" href="#Stream’s-Sparse-Recovery" class="headerlink" title="Stream’s Sparse Recovery"></a>Stream’s Sparse Recovery</h3><p>对于一个<script type="math/tex">k</script>-sparse 的数据流，我们可以设计算法来恢复其频数向量：</p><ol><li>首先设计一种方法，能够保证对于每个数据流中的值<script type="math/tex">v</script>，存在一个只保留了<script type="math/tex">v</script>相关特征的信息；</li><li>找到这个信息，还原<script type="math/tex">v</script>的特征。</li></ol><p>考虑构造<script type="math/tex">T = O(\log k)</script>个<script type="math/tex">[n] \to [2k]</script>的均匀哈希<script type="math/tex">h_1, \ldots, h_T</script>来统计每个值的出现次数。那么对于<script type="math/tex">v \in [n]</script>，存在<script type="math/tex">h_i</script>满足没有别的值与<script type="math/tex">v</script>冲突的概率很高。</p><p>具体地，我们可以用哈希维护三个量：<script type="math/tex">c_1(v) = \sharp v</script>、<script type="math/tex">c_2(v) = v\times \sharp v</script>，<script type="math/tex">c_3(v) = r^v\times \sharp v</script><sup><a id="reffn_11" class="headerlink" title="11"></a><a href="#fn_11">11</a></sup>。<script type="math/tex">r</script>是事先确定的随机值（fingerprint）。那么如果<script type="math/tex">c_1(v)</script>整除<script type="math/tex">c_2(v)</script>且<script type="math/tex">r^vc_1(v) = c_3(v)</script>，那么说明这个哈希的<script type="math/tex">v</script>没有与别的值冲突（只有整除并不一定能证明不冲突，需要用 fingerprint 来保证），并且我们可以计算<script type="math/tex">\frac{c_2(v)}{c_1(v)}</script>来得到<script type="math/tex">v</script>。事实上这是一个<script type="math/tex">1</script>-sparse 信息。</p><p>这算法事实上在不知道数据流是否<script type="math/tex">k</script>-sparse 的情况下也能写。而我们可以通过把恢复出来的信息删掉来检验原数据流是否<script type="math/tex">k</script>-sparse。</p><p>Linear Sketch：上述算法相当于是将频数向量进行特定的线性变换，并从<script type="math/tex">Ax</script>来还原<script type="math/tex">x</script>。其优点是具有线性可加性，也可以减。也就是说我们可以恢复一个区间的频数向量。</p><h3><a id="Compressed-Sensing" href="#Compressed-Sensing" class="headerlink" title="Compressed Sensing"></a>Compressed Sensing</h3><p>压缩感知是一种从低维信息还原高维的（稀疏的）信息的方法。</p><p>具体设定：找到一个<script type="math/tex">A\in \mathbf R^{m\times n}\; (m &lt; n)</script>使得对于任意<script type="math/tex">z \in \mathbf R^n</script>，设<script type="math/tex">b = Az</script>，那么我们总可以通过求解<script type="math/tex">Ax = b</script>来近似地找到<script type="math/tex">z</script>。<script type="math/tex">A</script>可以视为<script type="math/tex">m</script>个线性感知器（Linear Sketch）。</p><p>我们可以构造随机高斯矩阵<script type="math/tex">A</script>，然后使用线性规划 solver 暴力求解<script type="math/tex">x</script>。若<script type="math/tex">x</script>是<script type="math/tex">n</script>维<script type="math/tex">k</script>-sparse 向量，那么当<script type="math/tex">m = O(k\log \frac{n}{k})</script>时这种方法基本上可以完美还原。</p><h2><a id="ell-0-Sampling" href="#ell-0-Sampling" class="headerlink" title="\ell_0-Sampling"></a><script type="math/tex">\ell_0</script>-Sampling</h2><p><script type="math/tex">\ell_0</script>-Sampling 可以对于数据流返回一个<script type="math/tex">\operatorname{supp}(x)</script>上的均匀采样<script type="math/tex">p</script>，主要用于 support 比较大的情况。</p><p>一个对于<script type="math/tex">\operatorname{supp}(x)</script>的均匀采样可以这样得到：</p><ol><li>将<script type="math/tex">\operatorname{supp}(x)</script>中的元素以<script type="math/tex">\frac{1}{t}</script>的概率采样（以<script type="math/tex">\frac{1}{t}</script>的概率选择这个元素）；</li><li>在选出来的元素中均匀采样。</li></ol><p>事实上选出来的元素中只有一种值的概率是常数，因此可以用<script type="math/tex">1</script>-Sparse Recovery 来找到这个元素<sup><a id="reffn_9" class="headerlink" title="9"></a><a href="#fn_9">9</a></sup>。</p><p>因此我们分别取<script type="math/tex">t = 2^i</script>，<script type="math/tex">i = 1, \ldots, O(\log n)</script>构造采样哈希<script type="math/tex">h^{(i)}  : [n] \to \{ 0, 1\}</script>，满足<script type="math/tex">\Pr [h^{(i)} (p) = 1] = \frac{1}{2^i}</script>。然后任意找到其中的一个<script type="math/tex">1</script>-Sparse 做为采样结果。算法的空间复杂度<script type="math/tex">O(\operatorname{poly}(\log n))</script>，时间复杂度<script type="math/tex">O(\log n)</script>。以<script type="math/tex">1 - \frac{1}{\operatorname{poly}(n)}</script>的概率成功。重复若干次即可得到一个高成功的采样方法。但要注意，同样的采样哈希得到的采样是相同的，因此多次重复需要使用不同的哈希。</p><h3><a id="Stream-MST-Approximation" href="#Stream-MST-Approximation" class="headerlink" title="Stream MST Approximation"></a>Stream MST Approximation</h3><p>图数据流是指点集不变，边集是一个数据流的模型。</p><p>考虑在图数据流上求<strong>连通分量</strong>。</p><p>考虑类似 Boruvka 算法的过程：维护当前连通块集合，每次将每个连通块的跨越边找出，然后连起来，更新连通块信息。这个算法的可拓展性在于记录跨越边的方法。</p><p>考虑对于<script type="math/tex">u \in V</script>，定义当前边集<script type="math/tex">E</script>的频数向量为<script type="math/tex">x^u</script>，满足<script type="math/tex">x^u(u, v) = \operatorname{sgn}(u - v)</script>。定义<script type="math/tex">x^S = \sum_{u\in S} x^u</script>，那么可以发现<script type="math/tex">\operatorname{supp}(x^S)</script>恰好是<script type="math/tex">S</script>的跨越边。于是可以使用<script type="math/tex">\ell_0</script>采样的方式来找到跨越边。</p><p>当然我们可以沿用这个思路去求图数据流的<script type="math/tex">(1 + \varepsilon)</script>-<strong>近似 MST</strong>。</p><p>但是注意到我们只能做<script type="math/tex">\ell_0</script>均匀采样，而不能很方便地找最小值。这里考虑一个暴力的做法：将边权离散化，然后从小到大暴力枚举。</p><p>离散化：将边权 round 到最近的<script type="math/tex">(1 + \varepsilon)</script>的方幂。仔细想想是有道理的，并且可以显著降低不同边权个数<sup><a id="reffn_10" class="headerlink" title="10"></a><a href="#fn_10">10</a></sup>，也就将数据流稀疏化了。</p><p>离散化后就可以对每个边权<script type="math/tex">i</script>和每个结点<script type="math/tex">u</script>分别维护频数向量<script type="math/tex">x^{u, i}</script>。查询跨越边时从小到大枚举<script type="math/tex">i</script>，在<script type="math/tex">\sum _{u\in S} x^{u, i}</script>中查询即可。</p><p>复杂度分析：设有<script type="math/tex">L</script>种边权。对于每种边权我们构造一个可加的<script type="math/tex">\ell_0</script>采样，空间复杂度<script type="math/tex">O(nL\operatorname{poly}(\log |V|))</script>，找到跨越边的复杂度是<script type="math/tex">O(L\log n)</script>，总时间复杂度<script type="math/tex">O(nL\log^2 n)</script>。</p><h2><a id="Comments-amp-References" href="#Comments-amp-References" class="headerlink" title="Comments &amp; References"></a>Comments &amp; References</h2><p><a id="fn_1" class="headerlink" title="1"></a><sup>1</sup>. Jaccard Similarity，空集的情况为<script type="math/tex">0</script>​​​<a href="#reffn_1" title="Jump back to footnote [1] in the text."> &#8617;</a></p><p><a id="fn_2" class="headerlink" title="2"></a><sup>2</sup>. 有时也估计他们的 Cosine Similarity<a href="#reffn_2" title="Jump back to footnote [2] in the text."> &#8617;</a></p><p><a id="fn_3" class="headerlink" title="3"></a><sup>3</sup>.<script type="math/tex">\mathbf{H}</script>​​​ 为 Hamming 空间，距离为曼哈顿距离，这里的保度量是指夹角和 Hamming 距离<a href="#reffn_3" title="Jump back to footnote [3] in the text."> &#8617;</a></p><p><a id="fn_4" class="headerlink" title="4"></a><sup>4</sup>. 其实还需要再选择<script type="math/tex">2N</script>​​ 个 LCP 最大的去求最近邻，不过实践证明不太需要<a href="#reffn_4" title="Jump back to footnote [4] in the text."> &#8617;</a></p><p><a id="fn_5" class="headerlink" title="5"></a><sup>5</sup>. 这说明：对于近似算法来说，“⾼维” =<script type="math/tex">O(\log n)</script>​ 维<a href="#reffn_5" title="Jump back to footnote [5] in the text."> &#8617;</a></p><p><a id="fn_6" class="headerlink" title="6"></a><sup>6</sup>. 如何理解：<script type="math/tex">Xw=y</script>，则<script type="math/tex">w = X^{-1}y</script>，而<script type="math/tex">X</script>不可逆，则形式上构造<script type="math/tex">X^{-1}y = X^{-1}X^{-T}X^Ty=(X^TX)^{-1}X^Ty</script><a href="#reffn_6" title="Jump back to footnote [6] in the text.">&#8617;</a></p><p><a id="fn_7" class="headerlink" title="7"></a><sup>7</sup>. 个人理解：命名为 Spanner 的原因是<script type="math/tex">H</script>​ 是原图的一个生成子图<a href="#reffn_7" title="Jump back to footnote [7] in the text."> &#8617;</a></p><p><a id="fn_8" class="headerlink" title="8"></a><sup>8</sup>. ⼀般假设：频数向量每⼀维的最⼤绝对值是<script type="math/tex">O(\operatorname{poly}(n))</script>​​<a href="#reffn_8" title="Jump back to footnote [8] in the text."> &#8617;</a></p><p><a id="fn_9" class="headerlink" title="9"></a><sup>9</sup>. 1-Sparse 可以只需存那三个值<a href="#reffn_9" title="Jump back to footnote [9] in the text."> &#8617;</a></p><p><a id="fn_10" class="headerlink" title="10"></a><sup>10</sup>. 例如<script type="math/tex">\log_{1.1}(10^{10}) \approx 242</script><a href="#reffn_10" title="Jump back to footnote [10] in the text.">&#8617;</a></p><p><a id="fn_11" class="headerlink" title="11"></a><sup>11</sup>. 通过一些预处理方式可以<script type="math/tex">O(1)</script>​ 计算<script type="math/tex">r^v</script>​<a href="#reffn_11" title="Jump back to footnote [11] in the text."> &#8617;</a></p></div><hr><h3><span class="iconfont icon-history" style="padding-right:5px;font-size:1em"></span>修订记录</h3><div class="post-history-list"><ol reversed><li class="post-history-item">2023年6月23日 第2次修订</li><li class="post-history-item">2023年6月23日 创建文章</li></ol></div><div class="post-footer"><hr><div class="next-post"><span class="iconfont icon-arrow-left-circle"></span> <a href="/Data-Preprocess/">AI 系统速通笔记之数据预处理</a></div><div class="prev-post"><a href="/Math/Reflection-Inc-Exc/">反射容斥学习笔记 </a><span class="iconfont icon-arrow-right-circle"></span></div></div></article></main></body><div class="side-button"><div id="comment-button" class="button" title="Valine Comment"><span class="iconfont icon-comment"></span></div><div id="darkmode-button" class="button" title="Switch between day and night"><span class="iconfont icon-moonbyueliang"></span></div><div id="top" class="button" title="Back to top"><span class="iconfont icon-arrowup"></span></div></div><footer id="footer" style="margin:3em 0 2em 0;text-align:center;line-height:1.4em"><span class="post-wordcount iconfont icon-text"></span> 全站约 404,069 字<div class="license" style="padding:0 10px"><span>本站所有文章遵循许可协议 <a target="_blank" rel="license noreferrer noopener" href="https://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a></span></div><div class="credit"><span>Themed <a href="https://github.com/sshwy/hexo-theme-essence" target="_blank" rel="noreferrer noopener">Essence v1.9.11</a> | Powered by <a href="http://hexo.io" target="_blank" rel="noreferrer noopener">Hexo</a></span></div><div class="friends"><span class="friend-link-span">友情链接 </span><span class="icon friendsep"></span> <a target="_blank" rel="noopener" href="https://carol-xrl.github.io/"><span class="friend-link-span">Carol </span></a><span class="icon friendsep"></span> <a target="_blank" rel="noopener" href="https://www.cnblogs.com/GavinZheng"><span class="friend-link-span">GavinZheng </span></a><span class="icon friendsep"></span> <a target="_blank" rel="noopener" href="https://www.cnblogs.com/xryjr233"><span class="friend-link-span">xryjr233 </span></a><span class="icon friendsep"></span> <a target="_blank" rel="noopener" href="https://www.cnblogs.com/dysyn1314"><span class="friend-link-span" title="为什么我的头常常变绿？因为…… "><span style="font-weight:700"><span class="codeforces p">dy</span><span class="codeforces m">syn</span><span class="codeforces p">1314</span></span> </span></a><span class="icon friendsep"></span> <a target="_blank" rel="noopener" href="https://blog.aor.sd.cn"><span class="friend-link-span">RainAir </span></a><span class="icon friendsep"></span> <a target="_blank" rel="noopener" href="https://www.cnblogs.com/lcyfrog"><span class="friend-link-span">lcyfrog </span></a><span class="icon friendsep"></span> <a target="_blank" rel="noopener" href="https://www.cnblogs.com/maoyiting"><span class="friend-link-span">/hanx/se</span></a></div><div class="copyright"><span>Copyright &copy; 2019-present - Sshwy</span></div></footer><script src="/js/ssimple.js?20250423.js"></script><script src="https://pv.sohu.com/cityjson?ie=utf-8.js" defer></script></html>