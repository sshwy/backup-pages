<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=0" name="viewport"><meta name="keywords" content=""><meta name="description" content="Sshwy 的个人博客"><meta name="generator" content="Hexo 6.3.0"><title>Score-Based Diffusion 学习笔记 - Sshwy&#39;s Notes</title><link rel="shortcut icon" href="/images/favicon.ico"><link rel="stylesheet" href="/css/ssimple.css?20250423.css"><link rel="stylesheet" href="/iconfont/iconfont.css?20250423.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/katex.min.css"><script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/katex.min.js" crossorigin="anonymous"></script><script>document.addEventListener("DOMContentLoaded",function(){window.renderKatex()})</script></head><body><header><div class="header" style="width:100%"><div class="header-inner" style="position:fixed;z-index:20;top:0;left:0"><div class="header-content" style="max-width:1200px;margin:auto"><div class="blog-title"><span class="iconfont icon-menu1" id="menu-button"></span> <a href="/" class="logo header-title">Sshwy&#39;s Notes</a><div class="mobile-search"><input type="text"> <span class="iconfont icon-search mobile-search-icon"></span> <span class="iconfont icon-baseline-close-px search-close-icon"></span></div></div><div class="navbar" id="menu-list"><ul class="menu"><li class="menu-item"><a href="/archives/" class="menu-item-link"><span class="menu-item-icon iconfont icon-work"></span> Archives</a></li><li class="menu-item"><a href="/directory/" class="menu-item-link"><span class="menu-item-icon iconfont icon-folder-close"></span> Directory</a></li><li class="menu-item"><a href="/about/" class="menu-item-link"><span class="menu-item-icon iconfont icon-user"></span> About</a></li><li class="menu-item"><a href="/static/beibishi2023" class="menu-item-link"><span class="menu-item-icon iconfont icon-favorite"></span> NOI 背笔试</a></li></ul></div><div class="search"><input type="text"> <span class="iconfont icon-search search-icon"></span> <span class="iconfont icon-baseline-close-px search-close-icon"></span></div></div></div><div class="search-shadow"></div><div class="search-box"><div class="search-container"><div class="search-container-inner"><div class="search-data-status"><span>Fetching search data...</span></div><div class="search-count"></div><div class="search-result"></div></div></div></div></div></header><main class="main"><article class="post"><div class="post-title"><h1 class="page-title">Score-Based Diffusion 学习笔记</h1></div><div class="post-meta"><div class="post-info"><span class="post-info-item post-time" title="2023年11月25日星期六晚上8点26分 (CST+08:00)"><span class="info-icon iconfont icon-time"></span>更新于 2023年11月25日 </span><span class="icon infosep"></span><span class="post-info-item post-wordcount"> <span class="info-icon iconfont icon-text"></span>约 5,822 字</span></div><div class="post-directory"><span class="iconfont icon-folder-close"></span> <a class="directory" href="/directory">Home</a> <a class="directory" href="/directory/"></a> <span class="icon smallarrow"></span> 当前文章</div><div class="post-tags"><span class="tag"><span class="meta-icon iconfont icon-tag"></span><a class="tag" href="/tags/Notes/">Notes</a></span></div><hr></div><div class="toc"><h2 class="toc-title"><span class="iconfont icon-explain" style="font-size:1em;padding-right:5px"></span>文章目录</h2><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Score-Function"><span class="toc-number">1.</span> <span class="toc-text">Score Function</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Langevin-Algorithm1"><span class="toc-number">2.</span> <span class="toc-text">Langevin Algorithm1</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Generative-Modeling"><span class="toc-number">3.</span> <span class="toc-text">Generative Modeling</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Trainning-Objective"><span class="toc-number">4.</span> <span class="toc-text">Trainning Objective</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Denoise-Score-Matching4"><span class="toc-number">5.</span> <span class="toc-text">Denoise Score Matching4</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Annealed-Langevin-Dynamics"><span class="toc-number">6.</span> <span class="toc-text">Annealed Langevin Dynamics</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Stochastic-Differential-Equation"><span class="toc-number">7.</span> <span class="toc-text">Stochastic Differential Equation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#More-SDE"><span class="toc-number">8.</span> <span class="toc-text">More SDE</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Brownian-Motion"><span class="toc-number">8.1.</span> <span class="toc-text">Brownian Motion</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#The-Stochastic-Integral-of-Ito"><span class="toc-number">8.2.</span> <span class="toc-text">The Stochastic Integral of Itô</span></a></li></ol></li></ol></div><hr></div><div class="post-content"><script type="math/tex;mode=display">\gdef\vx{\mathbf{x}}
\gdef\vy{\mathbf{y}}
\gdef\vw{\mathbf{w}}
\gdef\vL{\mathbf{L}}
\gdef\vP{\mathbf{P}}
\gdef\vm{\mathbf{m}}
\gdef\vbeta{\boldsymbol{\beta}}
\gdef\vf{\mathbf{f}}
\gdef\md{\mathrm{d}}
\gdef\norm2#1{\left\Vert { #1 } \right\Vert_2}
\gdef\grad{\nabla}
\gdef\tr{\mathrm{tr}}</script><h2><a id="Score-Function" href="#Score-Function" class="headerlink" title="Score Function"></a>Score Function</h2><p><strong>Score function</strong>：对于<script type="math/tex">\mathbb{R}^n</script>上的概率密度函数（probability density function）为<script type="math/tex">p: \mathbb{R}^n \to \mathbb{R}</script>，则<script type="math/tex">p</script>之 score function 定义为其对数函数的梯度：</p><script type="math/tex;mode=display">s(\vx) = \frac{\partial \log p(\vx)}{\partial \vx} = \grad_{\vx} \log p(\vx) = \grad \log p(\vx)</script><p>（省略的梯度下标一般可以通过上下文直接推断）</p><p>这是 Score-Based 生成模型的核心概念。类比一个实函数的导数可以相当程度地刻画原函数的特征，那么 p. d. f. 之 score func. 也可以刻画原本的概率分布。</p><h2><a id="Langevin-Algorithm1" href="#Langevin-Algorithm1" class="headerlink" title="Langevin Algorithm1"></a>Langevin Algorithm<sup><a id="reffn_1" class="headerlink" title="1"></a><a href="#fn_1">1</a></sup></h2><p>该算法的名称有很多： Metropolis-adjusted Langevin algorithm (MALA) or Langevin Monte Carlo (LMC)。Langevin 算法可以利用 score function 来生成服从该分布的随机变量。用到的公式是（<script type="math/tex">\dot{\vx}</script>表示<script type="math/tex">\md\vx</script>）</p><script type="math/tex;mode=display">\dot{\vx} = \grad_{\vx}\log p(\vx) + \sqrt{2}\dot{\vw}</script><p>其中<script type="math/tex">\vw</script>表示一个标准布朗运动。使用离散化的算法实现它的方法有很多，比较简单的是 <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Euler–Maruyama_method">Euler–Maruyama method</a>：</p><script type="math/tex;mode=display">\vx(k + 1) = \vx(k) + \tau \grad p(\vx(k)) + \sqrt{2\tau} \xi(k)</script><p>其中<script type="math/tex">\xi(k)</script>服从<script type="math/tex">n</script>维高斯分布<script type="math/tex">\mathcal{N}_n(0, \mathbf{I})</script><sup><a id="reffn_2" class="headerlink" title="2"></a><a href="#fn_2">2</a></sup>。</p><h2><a id="Generative-Modeling" href="#Generative-Modeling" class="headerlink" title="Generative Modeling"></a>Generative Modeling</h2><p>生成模型可以理解为：输入的数据<script type="math/tex">\vx</script>服从某个先验分布<script type="math/tex">\pi</script>（例如均匀分布或者正态分布），输出的数据<script type="math/tex">\vy</script>服从目标分布<script type="math/tex">p_{\pi}</script>（例如人脸的分布）。因此生成模型的问题主要分为两个阶段：</p><ol><li>训练一个模型<script type="math/tex">f(\vx)</script>能够很好地表达分布<script type="math/tex">p_{\vy | \vx}(\vx)</script>的特征，例如去拟合<script type="math/tex">p</script>的 p. d. f.</li><li>根据模型<script type="math/tex">f</script>生成服从目标分布的值。</li></ol><p>而事实上，对于实际问题来说，目标分布常常由大量的样本点来代表，因此目标分布本身其实是一组样本点的最大似然。基于此，又可以对于生成模型的训练思路进一步分类：</p><ol><li>likelihood-based models：以最大似然估计为目标来拟合。</li><li>implicit generative models：使用一些别的最优化目标来拟合（比如 GAN）。</li></ol><p>Score-based 则代表另一种流派：拟合 p. d. f. 的梯度。同样的，我们面临两个阶段的问题：如何训练模型，以及如何生成服从目标分布的值。后者在上文中已经给出了一种思路，放在前面的原因是不涉及太多的背景知识。下文主要关注模型的训练。</p><h2><a id="Trainning-Objective" href="#Trainning-Objective" class="headerlink" title="Trainning Objective"></a>Trainning Objective</h2><p>任何模型的学习过程都是以最优化一个目标为导向。假设我们训练的模型是<script type="math/tex">s(\vx; \theta) = s_{\theta}(\vx)</script>，表示带有参数<script type="math/tex">\theta</script>的一个函数（分号用来区分自变量和参数）。那么一个最优的参数<script type="math/tex">\theta^{\ast}</script>即为</p><script type="math/tex;mode=display">\arg\min_{\theta} E_{p(\vx)}\left[\norm2{s_{\theta}(\vx) - \grad \log p(\vx)}^2\right]</script><p>更清楚地可以写成</p><script type="math/tex;mode=display">\arg\min_{\theta} \int_{\vx \in \mathbb{R}^n} p(\vx)\norm2{s_{\theta}(\vx) - \grad_{\vx} \log p(\vx)}^2\md\vx</script><p>如果<script type="math/tex">p</script>满足一些条件 <sup><a id="reffn_3" class="headerlink" title="3"></a><a href="#fn_3">3</a></sup>，那么我们可以把范数的部分写成离散求和的形式：</p><script type="math/tex;mode=display">\arg\min_{\theta} \int_{\vx \in \mathbb{R}^n} p(\vx)
\sum_{i = 1}^n
\left[
[\grad_{\vx} (s_{\theta}(\vx)_i)]_i + \frac{1}{2} s_{\theta}(\vx)_i^2
\right] \md\vx</script><p>可以发现<script type="math/tex">\grad \log p(\vx)</script>被消了，但是出现了一个<script type="math/tex">s_{\theta}(\vx)</script>的梯度（也就是 p. d. f. 的二阶梯度），这东西是一个矩阵，非常不好求。</p><p>考虑现实情况的<script type="math/tex">T</script>个样本点<script type="math/tex">\vx^{(1)}, \ldots, \vx^{(T)}</script>，那么上式的积分部分就变成了离散形式</p><script type="math/tex;mode=display">\arg\min_{\theta} 

\frac{1}{T}
\sum_{t = 1}^T
\sum_{i = 1}^n
\left[
[\grad_{\vx} (s_{\theta}(\vx^{(t)})_i)]_i + \frac{1}{2} s_{\theta}(\vx^{(t)})_i^2
\right]</script><p>简写一下可以变成</p><script type="math/tex;mode=display">\arg\min_{\theta} 

\frac{1}{T}
\sum_{t = 1}^T
\tr\left( \grad_{\vx} s_{\theta}(\vx^{(t)}) \right) + \frac{1}{2} \norm2{s_{\theta}(\vx^{(t)})}^2</script><p>这个式子显然还是没法直接求，对此有人开发了一套 score matching 理论来求解。</p><h2><a id="Denoise-Score-Matching4" href="#Denoise-Score-Matching4" class="headerlink" title="Denoise Score Matching4"></a>Denoise Score Matching<sup><a id="reffn_4" class="headerlink" title="4"></a><a href="#fn_4">4</a></sup></h2><p>这个方法的 Trainning Objective 是</p><script type="math/tex;mode=display">\frac{1}{2}  
E _{p_{\mathrm{data}}(\vx)}
E_{p_{\sigma}(\tilde\vx | \vx)}
\left[\norm2{
s_{\theta}(\tilde\vx) -
\grad_{\tilde\vx}\log p_{\sigma}(\tilde\vx | \vx)
}^2\right]</script><p>其中<script type="math/tex">p_{\sigma}(\tilde\vx | \vx) \triangleq \mathcal{N}_n(\tilde\vx; \vx, \sigma^2\mathbf{I})</script>是一个扰动核（perturbation kernel），是<script type="math/tex">\vx</script>的一个微小高斯扰动。<script type="math/tex">p_{\mathrm{data}}</script>表示数据分布。扰动后的数据分布可以表示为<script type="math/tex">p_{\sigma}(\tilde\vx) = \int_{\vx} p_{\sigma}(\tilde\vx | \vx)p_{\mathrm{data}}(\vx)\md\vx</script>。</p><p>对于这里的高斯分布，性质比较好，带入<script type="math/tex">\mathbf{\Sigma} = \sigma^2\mathbf{I}</script>可以写出它的数学形式<sup><a id="reffn_5" class="headerlink" title="5"></a><a href="#fn_5">5</a></sup></p><script type="math/tex;mode=display">p_{\sigma}(\tilde\vx | \vx) = 
\frac{\exp (-\frac{1}{2\sigma^2} \norm2{ \tilde\vx - \vx }^2 )}{\sqrt{(2\pi)^k\sigma^2}}</script><p>这样就可以自然推出</p><script type="math/tex;mode=display">\grad_{\tilde\vx} \log p_{\sigma} (\tilde\vx | \vx) =\frac{1}{\sigma^2}(\vx - \tilde\vx)</script><p>因此上面的 Objective 可以进一步写为</p><script type="math/tex;mode=display">\frac{1}{2}  
E _{p_{\mathrm{data}}(\vx)}
E_{p_{\sigma}(\tilde\vx | \vx)}
\left[\norm2{
s_{\theta}(\tilde\vx) -
\frac{1}{\sigma^2}(\vx - \tilde\vx)
}^2\right]</script><p>DSM 的 Objective 被证明，在<script type="math/tex">\sigma</script>充分小的时候，与原始的 Objective 在最优化的角度是等价的。</p><h2><a id="Annealed-Langevin-Dynamics" href="#Annealed-Langevin-Dynamics" class="headerlink" title="Annealed Langevin Dynamics"></a>Annealed Langevin Dynamics</h2><p>顾名思义，这是一个在 Langevin Dynamics 采样方法上加入了退火思想的一个变种。它想要解决的问题是：即使我们训练出一个优秀的<script type="math/tex">s_{\theta} \approx\grad_{\vx}\log p(\vx)</script>，也不一定能够在采样时得到目标分布。尤其是空间维度很大时，目标分布有时候会很稀疏，而如果迭代时的初始值位置不够好，就会很难在可接受的时间内收敛到对应的目标分布。</p><p>但注意到我们不一定非得一步到位。取<script type="math/tex">\sigma_1, \ldots, \sigma_L</script>为一个递减的序列（例如等比数列）。假设初始的随机变量<script type="math/tex">\vx</script>服从先验分布<script type="math/tex">\pi(\vx)</script>。我们首先使用 Langevin 算法将<script type="math/tex">\vx</script>转化为<script type="math/tex">p_{\sigma_1}(\vx)</script>的一个采样，再以此为基础去生成<script type="math/tex">p_{\sigma_2}(\vx)</script>的一个采样，以此类推，一步一步得到最终的目标分布<script type="math/tex">p_{\sigma_L}(\vx)</script>的一个采样。这可以理解为，从前一个分布到达下一个分布是相对简单的，因为初始值距离下一个目标分布足够“近”，有较大可能收敛。</p><p>这个采样方法需要我们求出<script type="math/tex">\grad_{\vx} \log p_{\sigma_i}(\vx)</script>（<script type="math/tex">i = 1, \ldots, L</script>），我们可以直接把<script type="math/tex">\sigma_i</script>作为网络的输入，训练出一个<script type="math/tex">s_{\theta}(\vx, \sigma) \approx \grad_{\vx} \log p_{\sigma}(\vx)</script>的模型。</p><h2><a id="Stochastic-Differential-Equation" href="#Stochastic-Differential-Equation" class="headerlink" title="Stochastic Differential Equation"></a>Stochastic Differential Equation</h2><p>从 Annealed Langevin Dynamics 进一步出发。当<script type="math/tex">L\to \infty</script>时，实际上<script type="math/tex">\sigma</script>就变成了一个连续变化的值。为了刻画<script type="math/tex">\sigma</script>的变化过程，我们可以把离散的迭代指标<script type="math/tex">i</script>替换为时间<script type="math/tex">t</script>，并不妨设<script type="math/tex">t \in[0, 1]</script>。那么<script type="math/tex">\sigma = \lambda(t)</script>就是一个随时间变化的扰动系数。引入了时间，很容易想到使用物理的方法去描述采样的过程。于是我们引入以下形式的随机微分方程：</p><script type="math/tex;mode=display">\md\vx = \vf(\vx, t)\md t + g(t)\md \vw</script><p>其中<script type="math/tex">\vw</script>是一个标准布朗运动（Wiener process，类似标准高斯分布），<script type="math/tex">\vf(\cdot, t)</script>称作平移系数（drift coef.），而<script type="math/tex">g(t)</script>称作扩散系数（diffusion coef.）。在 SBGM 的情境下，SDE 描述的是从一个数据分布<script type="math/tex">p_{\mathrm{data}}</script>出发，逐渐随机运动到先验分布的过程。不妨<script type="math/tex">p_t(\vx)</script>表示在<script type="math/tex">t</script>时刻<script type="math/tex">\vx</script>服从的分布，我们有<script type="math/tex">p_0 = p_{\mathrm{data}}</script>，<script type="math/tex">p_1 = \pi</script>。</p><p>要描述采样的过程，我们需要求解上述 SDE 的时间倒流过程<sup><a id="reffn_6" class="headerlink" title="6"></a><a href="#fn_6">6</a></sup>：</p><script type="math/tex;mode=display">\md\vx = [\vf(\vx, t) - g^2(t)\grad_{\vx} \log p_{t}(\vx)] \md t + g(t)\md \bar\vw</script><p>要训练<script type="math/tex">s_{\theta}(\vx, t) \approx \grad_{\vx} p_t(\vx)</script>，记<script type="math/tex">\vx(t)</script>表示<script type="math/tex">t</script>时刻的随机变量<script type="math/tex">\vx</script>，<script type="math/tex">p_{0t}</script>表示从<script type="math/tex">p_0</script>到<script type="math/tex">p_t</script>的变换核（即上文的扰动核），<script type="math/tex">\lambda(t)</script>是一个正的系数，那么最优的参数可以写成：</p><script type="math/tex;mode=display">\arg\min_{\theta}E_t
E _{\vx(0)}
E_{\vx(t) | \vx(0)}
\left[\lambda(t)\norm2{
s_{\theta}(\vx(t), t) -
\grad_{\vx(t)}\log p_{0t}(\vx(t) | \vx (0)) 
}^2\right]</script><p>当<script type="math/tex">\vf(\cdot, t) = 0</script>时，<script type="math/tex">p_{0t}</script>始终是一个高斯扰动（和<script type="math/tex">g</script>有关），此时 Trainning Objective 可以写成</p><script type="math/tex;mode=display">\min E_t E_{\vx(0)} E_{\vx(t) | \vx(0)}\left[ \lambda(t) \norm2{
s_{\theta}(\vx(t), t) -
\frac{\vx(0) - \vx(t)}{\sigma_{0t}^2}
}^2\right]</script><p>其中<script type="math/tex">\sigma_{0t}^2 = 1/ E\left[\norm2{\grad_{\vx(t)} \log p_{0t}(\vx(t) | \vx(0))}^2\right]</script>表示<script type="math/tex">p_{0t}(\vx(t) | \vx(0))</script>的方差。另外论文中<script type="math/tex">E\left[\norm2{\grad_{\vx(t)} \log p_{0t}(\vx(t) | \vx(0))}^2\right] = \frac{1}{\sigma_{0t}^2}</script>。</p><p>取<script type="math/tex">\lambda(t) \propto \sigma^2_{0t}</script>就会得到</p><script type="math/tex;mode=display">\min E_t E_{\vx(0)} E_{\vx(t) | \vx(0)} \norm2{
s_{\theta}(\vx(t), t)\sigma_{0t} + \frac{\vx(t) - \vx(0)}{\sigma_{0t}}
}^2</script><p>并且由经验可以得出<script type="math/tex">\norm2{\sigma_{0t} s_{\theta}(\vx, t)} \propto 1</script>. 因此在训练神经网络时可以手动 normalize.</p><p>此时可以得到原本的 SDE 为</p><script type="math/tex;mode=display">\md\vx = \sqrt{\lambda(t)}\md\vw</script><p>对应的 Reverse SDE：</p><script type="math/tex;mode=display">\md \vx = -\lambda(t)\grad_{\vx}\log p_t(\vx)\md t + \sqrt{\lambda(t)}\md\bar\vw</script><h2><a id="More-SDE" href="#More-SDE" class="headerlink" title="More SDE"></a>More SDE</h2><p>接下来我们探索一些 SDE 的具体内容，可能会作为上文的引用。主要参考 Applied Stochastic Differential Equation<sup><a id="reffn_7" class="headerlink" title="7"></a><a href="#fn_7">7</a></sup>。</p><h3><a id="Brownian-Motion" href="#Brownian-Motion" class="headerlink" title="Brownian Motion"></a>Brownian Motion</h3><p>首先我们给出布朗运动（Brownian motion）的定义：<script type="math/tex">\vbeta(t) \in \mathbb{R}^n</script>是一个具有以下性质的随机过程：</p><ol><li><script type="math/tex">\Delta\vbeta_k = \vbeta(t_{k + 1}) - \vbeta(t_k)</script>服从一个均值为<script type="math/tex">0</script>，协方差<sup><a id="reffn_8" class="headerlink" title="8"></a><a href="#fn_8">8</a></sup>矩阵为<script type="math/tex">\mathbf{Q}(t_{k + 1} - t_k)</script>的高斯分布，其中<script type="math/tex">\mathbf{Q}</script>是布朗运动的扩散矩阵（diffusion matrix）。</li><li>时间段不相交的<script type="math/tex">\Delta \vbeta_k</script>是独立的。</li><li><script type="math/tex">\vbeta(0) = 0</script>。</li></ol><p>关于它有两点需要注意：<script type="math/tex">\vbeta(t)</script>处处不可微，不过白噪声可以作为布朗运动的形式上的导数：<script type="math/tex">\vw(t) = \frac{\md \vbeta(t)}{\md t}</script>。</p><h3><a id="The-Stochastic-Integral-of-Ito" href="#The-Stochastic-Integral-of-Ito" class="headerlink" title="The Stochastic Integral of Itô"></a>The Stochastic Integral of Itô</h3><p>一个随机过程，在任何时刻运动到的位置和速度等等都没有上下界，不是一个确定性的收敛的过程，因此没法用黎曼积分去刻画。</p><p>SDE 所对应的积分过程是 The Stochastic Integral of Itô，定义为<script type="math/tex">L_2</script>意义下的极限：</p><script type="math/tex;mode=display">\int_{t_0}^t \vL(\vx(t), t) \md\vbeta(t) = 
\lim_{n\to \infty} \sum_k \vL(\vx(t), t)[\vbeta(t_{k + 1}) - \vbeta(t_k)]</script><p>其中<script type="math/tex">t_0 &lt; t_1 &lt; \cdots &lt; t_n = t</script>，<script type="math/tex">\vL</script>是一个矩阵函数。注意到这个积分不像黎曼积分，在分划的小区间里面随便取一个值。如果这样做其实会导致积分不收敛。</p><p>这个积分也解释了上文<script type="math/tex">\vbeta(t)</script>的形式导数的具体含义。因此随机微分方程</p><script type="math/tex;mode=display">\md\vx = \vf(\vx, t)\md t + \vL(\vx, t)\md \vbeta</script><p>的具体定义也就清楚了，也可以写成</p><script type="math/tex;mode=display">\frac{\md\vx}{\md t} = \vf(\vx, t) +\vL(\vx, t) \frac{\md \vbeta}{\md t}</script><p><a id="fn_1" class="headerlink" title="1"></a><sup>1</sup>. <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Metropolis-adjusted_Langevin_algorithm#">https://en.wikipedia.org/wiki/Metropolis-adjusted_Langevin_algorithm#</a><a href="#reffn_1" title="Jump back to footnote [1] in the text."> &#8617;</a></p><p><a id="fn_2" class="headerlink" title="2"></a><sup>2</sup>. <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Multivariate_normal_distribution">https://en.wikipedia.org/wiki/Multivariate_normal_distribution</a><a href="#reffn_2" title="Jump back to footnote [2] in the text."> &#8617;</a></p><p><a id="fn_3" class="headerlink" title="3"></a><sup>3</sup>. Estimation of non-normalized statistical models by score matching. A. Hyvarinen. <a target="_blank" rel="noopener" href="https://www.jmlr.org/papers/volume6/hyvarinen05a/hyvarinen05a.pdf">https://www.jmlr.org/papers/volume6/hyvarinen05a/hyvarinen05a.pdf</a><a href="#reffn_3" title="Jump back to footnote [3] in the text."> &#8617;</a></p><p><a id="fn_4" class="headerlink" title="4"></a><sup>4</sup>. A Connection Between Score Matching and Denoising Autoencoders. P. Vincent. <a target="_blank" rel="noopener" href="https://www.iro.umontreal.ca/~vincentp/Publications/smdae_techreport.pdf">https://www.iro.umontreal.ca/~vincentp/Publications/smdae_techreport.pdf</a><a href="#reffn_4" title="Jump back to footnote [4] in the text."> &#8617;</a></p><p><a id="fn_5" class="headerlink" title="5"></a><sup>5</sup>. <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Non-degenerate_case">https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Non-degenerate_case</a><a href="#reffn_5" title="Jump back to footnote [5] in the text."> &#8617;</a></p><p><a id="fn_6" class="headerlink" title="6"></a><sup>6</sup>. Reverse-Time Diffusion Equation Models. Brian D.O. Anderson. <a target="_blank" rel="noopener" href="https://core.ac.uk/download/pdf/82826666.pdf">https://core.ac.uk/download/pdf/82826666.pdf</a><a href="#reffn_6" title="Jump back to footnote [6] in the text."> &#8617;</a></p><p><a id="fn_7" class="headerlink" title="7"></a><sup>7</sup>. Simo Särkkä and Arno Solin (2019). Applied Stochastic Differential Equations. Cambridge University Press. <a target="_blank" rel="noopener" href="https://users.aalto.fi/~asolin/sde-book/sde-book.pdf">https://users.aalto.fi/~asolin/sde-book/sde-book.pdf</a><a href="#reffn_7" title="Jump back to footnote [7] in the text."> &#8617;</a></p><p><a id="fn_8" class="headerlink" title="8"></a><sup>8</sup>. <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Covariance">https://en.wikipedia.org/wiki/Covariance</a><a href="#reffn_8" title="Jump back to footnote [8] in the text."> &#8617;</a></p><p><a id="fn_9" class="headerlink" title="9"></a><sup>9</sup>. <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Partial_derivative#Definition">https://en.wikipedia.org/wiki/Partial_derivative#Definition</a><a href="#reffn_9" title="Jump back to footnote [9] in the text."> &#8617;</a></p></div><hr><h3><span class="iconfont icon-history" style="padding-right:5px;font-size:1em"></span>修订记录</h3><div class="post-history-list"><ol reversed><li class="post-history-item">2023年11月25日 第3次修订</li><li class="post-history-item">2023年5月29日 第2次修订</li><li class="post-history-item">2023年5月28日 创建文章</li></ol></div><div class="post-footer"><hr><div class="next-post"><span class="iconfont icon-arrow-left-circle"></span> <a href="/Math/Burnside-and-Polya/">Burnside 引理入门</a></div><div class="prev-post"><a href="/Math/Stirling-Number/">斯特林数学习笔记 </a><span class="iconfont icon-arrow-right-circle"></span></div></div></article></main></body><div class="side-button"><div id="comment-button" class="button" title="Valine Comment"><span class="iconfont icon-comment"></span></div><div id="darkmode-button" class="button" title="Switch between day and night"><span class="iconfont icon-moonbyueliang"></span></div><div id="top" class="button" title="Back to top"><span class="iconfont icon-arrowup"></span></div></div><footer id="footer" style="margin:3em 0 2em 0;text-align:center;line-height:1.4em"><span class="post-wordcount iconfont icon-text"></span> 全站约 404,069 字<div class="license" style="padding:0 10px"><span>本站所有文章遵循许可协议 <a target="_blank" rel="license noreferrer noopener" href="https://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a></span></div><div class="credit"><span>Themed <a href="https://github.com/sshwy/hexo-theme-essence" target="_blank" rel="noreferrer noopener">Essence v1.9.11</a> | Powered by <a href="http://hexo.io" target="_blank" rel="noreferrer noopener">Hexo</a></span></div><div class="friends"><span class="friend-link-span">友情链接 </span><span class="icon friendsep"></span> <a target="_blank" rel="noopener" href="https://carol-xrl.github.io/"><span class="friend-link-span">Carol </span></a><span class="icon friendsep"></span> <a target="_blank" rel="noopener" href="https://www.cnblogs.com/GavinZheng"><span class="friend-link-span">GavinZheng </span></a><span class="icon friendsep"></span> <a target="_blank" rel="noopener" href="https://www.cnblogs.com/xryjr233"><span class="friend-link-span">xryjr233 </span></a><span class="icon friendsep"></span> <a target="_blank" rel="noopener" href="https://www.cnblogs.com/dysyn1314"><span class="friend-link-span" title="为什么我的头常常变绿？因为…… "><span style="font-weight:700"><span class="codeforces p">dy</span><span class="codeforces m">syn</span><span class="codeforces p">1314</span></span> </span></a><span class="icon friendsep"></span> <a target="_blank" rel="noopener" href="https://blog.aor.sd.cn"><span class="friend-link-span">RainAir </span></a><span class="icon friendsep"></span> <a target="_blank" rel="noopener" href="https://www.cnblogs.com/lcyfrog"><span class="friend-link-span">lcyfrog </span></a><span class="icon friendsep"></span> <a target="_blank" rel="noopener" href="https://www.cnblogs.com/maoyiting"><span class="friend-link-span">/hanx/se</span></a></div><div class="copyright"><span>Copyright &copy; 2019-present - Sshwy</span></div></footer><script src="/js/ssimple.js?20250423.js"></script><script src="https://pv.sohu.com/cityjson?ie=utf-8.js" defer></script></html>