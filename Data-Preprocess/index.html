<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=0" name="viewport"><meta name="keywords" content=""><meta name="description" content="Sshwy 的个人博客"><meta name="generator" content="Hexo 6.3.0"><title>AI 系统速通笔记之数据预处理 - Sshwy&#39;s Notes</title><link rel="shortcut icon" href="/images/favicon.ico"><link rel="stylesheet" href="/css/ssimple.css?20250423.css"><link rel="stylesheet" href="/iconfont/iconfont.css?20250423.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/katex.min.css"><script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/katex.min.js" crossorigin="anonymous"></script><script>document.addEventListener("DOMContentLoaded",function(){window.renderKatex()})</script></head><body><header><div class="header" style="width:100%"><div class="header-inner" style="position:fixed;z-index:20;top:0;left:0"><div class="header-content" style="max-width:1200px;margin:auto"><div class="blog-title"><span class="iconfont icon-menu1" id="menu-button"></span> <a href="/" class="logo header-title">Sshwy&#39;s Notes</a><div class="mobile-search"><input type="text"> <span class="iconfont icon-search mobile-search-icon"></span> <span class="iconfont icon-baseline-close-px search-close-icon"></span></div></div><div class="navbar" id="menu-list"><ul class="menu"><li class="menu-item"><a href="/archives/" class="menu-item-link"><span class="menu-item-icon iconfont icon-work"></span> Archives</a></li><li class="menu-item"><a href="/directory/" class="menu-item-link"><span class="menu-item-icon iconfont icon-folder-close"></span> Directory</a></li><li class="menu-item"><a href="/about/" class="menu-item-link"><span class="menu-item-icon iconfont icon-user"></span> About</a></li><li class="menu-item"><a href="/static/beibishi2023" class="menu-item-link"><span class="menu-item-icon iconfont icon-favorite"></span> NOI 背笔试</a></li></ul></div><div class="search"><input type="text"> <span class="iconfont icon-search search-icon"></span> <span class="iconfont icon-baseline-close-px search-close-icon"></span></div></div></div><div class="search-shadow"></div><div class="search-box"><div class="search-container"><div class="search-container-inner"><div class="search-data-status"><span>Fetching search data...</span></div><div class="search-count"></div><div class="search-result"></div></div></div></div></div></header><main class="main"><article class="post"><div class="post-title"><h1 class="page-title">AI 系统速通笔记之数据预处理</h1></div><div class="post-meta"><div class="post-info"><span class="post-info-item post-time" title="2023年5月30日星期二晚上10点30分 (CST+08:00)"><span class="info-icon iconfont icon-time"></span>更新于 2023年5月30日 </span><span class="icon infosep"></span><span class="post-info-item post-wordcount"> <span class="info-icon iconfont icon-text"></span>约 3,685 字</span></div><div class="post-directory"><span class="iconfont icon-folder-close"></span> <a class="directory" href="/directory">Home</a> <a class="directory" href="/directory/"></a> <span class="icon smallarrow"></span> 当前文章</div><hr></div><div class="toc"><h2 class="toc-title"><span class="iconfont icon-explain" style="font-size:1em;padding-right:5px"></span>文章目录</h2><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#概述"><span class="toc-number">1.</span> <span class="toc-text">概述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#箱线图1"><span class="toc-number">2.</span> <span class="toc-text">箱线图1</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#信息熵"><span class="toc-number">3.</span> <span class="toc-text">信息熵</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#决策树"><span class="toc-number">4.</span> <span class="toc-text">决策树</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#划分准则"><span class="toc-number">4.1.</span> <span class="toc-text">划分准则</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#回归树"><span class="toc-number">4.2.</span> <span class="toc-text">回归树</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#数值规范化"><span class="toc-number">5.</span> <span class="toc-text">数值规范化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#特征选择"><span class="toc-number">6.</span> <span class="toc-text">特征选择</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Pearson-相关系数"><span class="toc-number">6.1.</span> <span class="toc-text">Pearson 相关系数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#嵌入法"><span class="toc-number">6.2.</span> <span class="toc-text">嵌入法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#迭代法"><span class="toc-number">6.3.</span> <span class="toc-text">迭代法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Gini-Importance3"><span class="toc-number">6.4.</span> <span class="toc-text">Gini Importance3</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Permutation-Importance"><span class="toc-number">6.5.</span> <span class="toc-text">Permutation Importance</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#SHAP6"><span class="toc-number">6.6.</span> <span class="toc-text">SHAP6</span></a></li></ol></li></ol></div><hr></div><div class="post-content"><p>本文的标题非常不负责任，概 AI 基础这门课本身的课程内容极具争议的缘故。</p><p>拿到了数据后要先进行一些预处理才会拿去做模型训练。这里面的门道比想象中多。</p><script type="math/tex;mode=display">\gdef\vx{\mathbf{x}}</script><h2><a id="概述" href="#概述" class="headerlink" title="概述"></a>概述</h2><p>首先我们理清数据预处理的流程，用到的比较具体的技术和方法会在下面专门讲。</p><p>首先，我们需要清除一些错误的数据。一个比较有效的统计方法是使用箱线图来去除极端的值。</p><p>有些数据的特征可能含有缺失项，如果确实不具备参考价值可以直接删除（pandas 中可以调用 <code class="inline-code">dropna</code>）。但是如果要利用含有缺失项的特征，我们需要适当给予补充修复。常见的做法是使用已有数据平均值/中位数/默认值/相邻的值补全，或者采用某种公式插值（线性插值，例如按照时间排序的温度信息）。</p><p>接下来我们需要做一些数据转化。对于实数特征我们可以按照某种公式进行规范化，对于图片我们可以裁剪、压缩等等，对于文本数据可以做分词和词的分类（英文单词的不同变形）等等。</p><p>在这之后，我们可以使用特征工程的一些方法来筛选、简化、合并特征，使得之后的（深度）学习过程更加容易。为此我们主要进行特征选择，很多特征选择的方法也会对应特征的合并或者训练上的调整。</p><h2><a id="箱线图1" href="#箱线图1" class="headerlink" title="箱线图1"></a>箱线图<sup><a id="reffn_1" class="headerlink" title="1"></a><a href="#fn_1">1</a></sup></h2><p>箱线图主要用于检测数据中的离群值。</p><p>设<script type="math/tex">Q_2</script>表示所有数值的中位数，<script type="math/tex">Q_1</script>表示小于<script type="math/tex">Q_2</script>这部分值的中位数（第一四分位数），<script type="math/tex">Q_3</script>为大于<script type="math/tex">Q_2</script>这部分值的四分位数（第二四分位数）。<script type="math/tex">[Q_1, Q_3]</script>就是箱子的范围。</p><p>称<script type="math/tex">Q_3 - Q_1 = \mathrm{IQR}</script>为四分位间距 Interquartile range。则<script type="math/tex">[Q_1 - 1.5\mathrm{IQR}, Q_3 + 1.5\mathrm{IQR}]</script>就是线段的范围。</p><p>落在线段之外的值就是离群值（outlier）。下面就是本次作业的房子数据各个特征的箱线图，可以发现确实有许多离群值：</p><p><img src="../images/data_preprocess_1.png" alt=""></p><p>用到的库是 python 的 <code class="inline-code">seaborn</code>。</p><h2><a id="信息熵" href="#信息熵" class="headerlink" title="信息熵"></a>信息熵</h2><p>首先我们给出<strong>信息熵</strong>的定义：对于随机变量<script type="math/tex">X</script>，和其服从的概率分布<script type="math/tex">p</script>，定义</p><script type="math/tex;mode=display">H(X) = -\sum_{x} p(x)\log p(x)</script><p>信息熵可以理解为系统内不同事件信息量的期望。</p><p>对于两个随机变量<script type="math/tex">X, Y</script>和其联合分布，我们可以定义<strong>联合熵</strong>：</p><script type="math/tex;mode=display">H(X, Y) = -\sum_{x, y} p(x, y)\log p(x, y)</script><p>它描述一个联合分布<script type="math/tex">p(x, y)</script>的不确定程度。此外，已知<script type="math/tex">X</script>时<script type="math/tex">Y</script>的不确定程度可以用<strong>条件熵</strong>表征：</p><script type="math/tex;mode=display">\begin{align}
H(Y | X) 
&= \sum_{x} p(x) H(Y | X = x) \\
&= -\sum_{x} p(x) \sum_y p(y | x) \log p(y | x) \\
&= -\sum_{x, y} p(x, y) \log p(y | x) \\
\end{align}</script><p>条件熵与联合熵的关系是<script type="math/tex">H(X, Y) = H(Y) + H(X | Y) = H(X) + H(Y | X) = H(Y, X)</script>，可以通过推导证明。有了熵，我们就可以定义<strong>互信息</strong></p><script type="math/tex;mode=display">I(X, Y) = H(X) + H(Y) - H(X, Y)</script><p>结合一下条件熵与联合熵的关系，可以发现<script type="math/tex">I(X, Y) = H(X) - H(X | Y) = H(Y) - H(Y | X)</script>，可以画出直观易懂的维恩图。</p><h2><a id="决策树" href="#决策树" class="headerlink" title="决策树"></a>决策树</h2><p>决策树可以将一组数据按照其特征进行分类，其中</p><ul><li>根节点包含所有数据</li><li>每个非叶子节点中的数据会按照某个属性进行分类，将分类的数据放在子节点中</li><li>每个叶子节点对应决策结果</li></ul><p>要构建决策树，主要考虑的问题是选择什么属性来划分数据最优。直观上我们希望划分后尽量是“均分”的，而不是所有数据归为一类。为了衡量划分的优秀性，我们可以结合上文提到的信息熵。</p><h3><a id="划分准则" href="#划分准则" class="headerlink" title="划分准则"></a>划分准则</h3><p>我们可以使用<strong>信息增益</strong>来度量划分的优秀性。为此首先我们要度量数据集<script type="math/tex">D</script>在分类标签集<script type="math/tex">K</script>下的<strong>纯度</strong>：</p><script type="math/tex;mode=display">H_K(D) = -\sum_{k \in K} \frac{|D_k|}{|D|} \log \frac{|D_k|}{|D|}</script><p>其中<script type="math/tex">D_k</script>表示标签为<script type="math/tex">k</script>的数据集合，<script type="math/tex">\frac{|D_k|}{|D|}</script>自然表示划分到<script type="math/tex">k</script>类别所占的比例。<script type="math/tex">H(D)</script>越高意味着样本纯度越低。</p><p>特征<script type="math/tex">A</script>对<script type="math/tex">D</script>的信息增益定义为</p><script type="math/tex;mode=display">g(D, A) = H_K(D) - \sum_a \frac{|D^{A=a}|}{|D|} H_K(D^{A=a})</script><p>单从实现的角度，<script type="math/tex">H_K(D)</script>可以不管。后面的和式描述的是特征<script type="math/tex">A</script>的每个取值下样本的纯度。显然纯度越有利于分类，因此我们取一个负号在前面。</p><p>信息增益还可以理解为属性与标签之间的互信息。</p><p>但是信息增益的问题是如果<script type="math/tex">A</script>的取值过多，每个取值下的数据过少，这样的分类效率和泛化性都不高。</p><p>我们还可以采用<strong>增益率</strong>作为划分准则：</p><script type="math/tex;mode=display">g_R(D, A) = \frac{g(D, A)}{H_A(D)}</script><p>这样就缓解了上述问题。</p><p>另一种划分准则是<strong>基尼指数</strong>（Gini index，也称 Gini Impurity）：</p><script type="math/tex;mode=display">\mathrm{Gini}(D) =\sum_{k \in K} \frac{|D_k|}{|D|}\left(1- \frac{|D_k|}{|D|}\right) 
= 1 - \sum_{k\in K} \left( \frac{|D_k|}{|D|} \right)^2</script><p>特征<script type="math/tex">A</script>对<script type="math/tex">D</script>的 Gini 指数为</p><script type="math/tex;mode=display">\mathrm{Gini}(D, A) = \sum_{a \in A} \frac{|D^{A = a}|}{|D|}\mathrm{Gini}(D^{A=a})</script><p><script type="math/tex">\mathrm{Gini}(D, A)</script>越小，意味着以特征<script type="math/tex">A</script>划分后信息量越少，说明这种划分对提升数据纯度的帮助越大。</p><p>事实上我们可以类比信息增益定义 Gini Decrease 为<script type="math/tex">\mathrm{Gini}(D) - \mathrm{Gini}(D, A)</script>。</p><h3><a id="回归树" href="#回归树" class="headerlink" title="回归树"></a>回归树</h3><p>上面描述的都是利用离散的特征对数据分类的问题。</p><p>如果特征是连续的值，我们可以将其按照某种分度离散化。</p><p>如果目标是对数据的标签做回归分析（例如房价预测），我们只需要稍微魔改一下决策树就能解决这个问题。为此我们引入回归树。</p><p>回归树与决策树结构相同，但是对于集合纯度我们使用<script type="math/tex">L_2</script>估计。设<script type="math/tex">L(D)</script>表示数据集的方差，<script type="math/tex">L(D, A) = \sum_a L(D^{A = a})</script>，那么最优特征定义为</p><script type="math/tex;mode=display">A^{\ast} = \arg\min_{A} L(D, A)</script><p>决策树和回归树在 <code class="inline-code">sklearn.tree</code> 中均有对应的 API 可以调用。</p><h2><a id="数值规范化" href="#数值规范化" class="headerlink" title="数值规范化"></a>数值规范化</h2><p>常见的规范函数有</p><ul><li><p>线性映射为<script type="math/tex">[a, b]</script>之间的值（min-max normalization）：</p><script type="math/tex;mode=display">x_i' = \frac{x_i - \min}{\max - \min}(b - a) + a</script></li><li><p>映射成均值为<script type="math/tex">0</script>，方差为<script type="math/tex">1</script>的数据分布（z-score normalization）：</p><script type="math/tex;mode=display">x_i' = \frac{x_i - \bar{x}}{\mathrm{std}(x)}</script></li><li><p>十进制缩放（decimal scaling）：</p><script type="math/tex;mode=display">x_i' = \frac{x_i}{10^j}</script><p>其中<script type="math/tex">j</script>为使得<script type="math/tex">\max_i |x_i'| &lt; 1</script>的最小的<script type="math/tex">j</script>。</p></li><li><p>对数缩放（log scaling）：</p><script type="math/tex;mode=display">x_i' = \log x_i</script></li></ul><h2><a id="特征选择" href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h2><p>特征选择解决问题是：当可用的特征过多时，选择其中最合适的作为子集，最大化训练模型的效率。首先我们可以使用 Pearson 相关系数、互信息、Gini index 等方法来判断两个特征的相关程度。我们也可以将特征选择的过程和模型训练结合到一起，也就是嵌入法。还可以将特征选择看成搜索问题。</p><h3><a id="Pearson-相关系数" href="#Pearson-相关系数" class="headerlink" title="Pearson 相关系数"></a>Pearson 相关系数</h3><p>中学知识：</p><script type="math/tex;mode=display">r = \frac{\sum(x - \bar x) (y - \bar y)}{\mathrm{std}(x)\mathrm{std}(y)}</script><p>在 <code class="inline-code">scipy</code> 中有对应的 API。</p><h3><a id="嵌入法" href="#嵌入法" class="headerlink" title="嵌入法"></a>嵌入法</h3><p>对于一个回归问题，以线性回归为例，特征可以抽象为<script type="math/tex">d</script>维向量<script type="math/tex">\vx</script>，回归目标为<script type="math/tex">y</script>。对于多个数据<script type="math/tex">(\vx_1, y_1), \ldots, (\vx_n, y_n)</script>，目标函数是<script type="math/tex">\min\sum(y_i - \hat{y}_i)^2</script>。使用的模型是<script type="math/tex">\hat{y}_i =\mathbf{w}^T\vx_i</script>。</p><p>而 Lasso 方法则在线性回归的基础上对<script type="math/tex">\mathbf{w}</script>的值进行<script type="math/tex">L_1</script>正则化：</p><script type="math/tex;mode=display">\min \sum (y_i -\hat{y_i})^2 + \theta \sum_{k = 1}^n |w_i|</script><p>在<script type="math/tex">L_1</script>正则化下，大部分系数会是一个接近<script type="math/tex">0</script>的数字，剩下的绝对值很大的系数对应的是重要的特征。这种方法统称嵌入法。</p><p>值得一提的是，Ridge<sup><a id="reffn_2" class="headerlink" title="2"></a><a href="#fn_2">2</a></sup> 回归也使用了相似的思想，即在最小化损失的同时对参数进行规范化，如果要用它做特征选择可能会需要做一些矩阵运算。</p><h3><a id="迭代法" href="#迭代法" class="headerlink" title="迭代法"></a>迭代法</h3><p>如果将特征选择看成搜索问题，暴力搜索有<script type="math/tex">2^N</script>种情况，十分不优秀。</p><p>此时我们可以使用贪心的方法，从空集开始每次加入一个特征，并计算模型的性能提升程度（递增）；或者从全集开始每次减少一个特征，并计算模型的性能降低程度（递减）。</p><h3><a id="Gini-Importance3" href="#Gini-Importance3" class="headerlink" title="Gini Importance3"></a>Gini Importance<sup><a id="reffn_3" class="headerlink" title="3"></a><a href="#fn_3">3</a></sup></h3><p>也称 Mean Decrease in Impurity (MDI)。Gini Importance 的实现方式目前存在争议。这里我们讲的是 <code class="inline-code">sklearn</code> 的实现方法<sup><a id="reffn_4" class="headerlink" title="4"></a><a href="#fn_4">4</a></sup><sup><a id="reffn_5" class="headerlink" title="5"></a><a href="#fn_5">5</a></sup>。</p><p>设决策树的结点集合为<script type="math/tex">T</script>，<script type="math/tex">D_u</script>表示结点<script type="math/tex">u</script>上的数据，<script type="math/tex">A_u</script>表示<script type="math/tex">u</script>上用于分割的特征。那么特征<script type="math/tex">A</script>的 Gini Importance 定义为</p><script type="math/tex;mode=display">\sum_{u}[A_u = A] [\mathrm{Gini}(D_u) - \mathrm{Gini}(D_u, A_u)]\frac{|D_u|}{|D|}</script><h3><a id="Permutation-Importance" href="#Permutation-Importance" class="headerlink" title="Permutation Importance"></a>Permutation Importance</h3><p>也称 Mean Decrease in Accuracy (MDA)。</p><p>我们可以将数据的某个特征打乱顺序，计算打乱前和打乱后的准确性<script type="math/tex">a_0, a_1</script>，那么这个特征的重排重要性就是<script type="math/tex">\frac{a_0 - a_1}{a_0}</script>，以此来估计哪些特征具有保留价值。</p><p>在 <code class="inline-code">sklearn.inspection</code> 中有对应的 API。</p><h3><a id="SHAP6" href="#SHAP6" class="headerlink" title="SHAP6"></a>SHAP<sup><a id="reffn_6" class="headerlink" title="6"></a><a href="#fn_6">6</a></sup></h3><p>SHAP 是一个用于特征解释的工具，也可以用于特征选择。</p><p><a target="_blank" rel="noopener" href="https://christophm.github.io/interpretable-ml-book/shapley.html">相关连接</a></p><p><a id="fn_1" class="headerlink" title="1"></a><sup>1</sup>. <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Box_plot">https://en.wikipedia.org/wiki/Box_plot</a><a href="#reffn_1" title="Jump back to footnote [1] in the text."> &#8617;</a></p><p><a id="fn_2" class="headerlink" title="2"></a><sup>2</sup>. <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Ridge_regression">https://en.wikipedia.org/wiki/Ridge_regression</a><a href="#reffn_2" title="Jump back to footnote [2] in the text."> &#8617;</a></p><p><a id="fn_3" class="headerlink" title="3"></a><sup>3</sup>. <a target="_blank" rel="noopener" href="https://medium.com/the-artificial-impostor/feature-importance-measures-for-tree-models-part-i-47f187c1a2c3">https://medium.com/the-artificial-impostor/feature-importance-measures-for-tree-models-part-i-47f187c1a2c3</a><a href="#reffn_3" title="Jump back to footnote [3] in the text."> &#8617;</a></p><p><a id="fn_4" class="headerlink" title="4"></a><sup>4</sup>. <a target="_blank" rel="noopener" href="https://scikit-learn.org/stable/modules/tree.html#tree-mathematical-formulation">https://scikit-learn.org/stable/modules/tree.html#tree-mathematical-formulation</a><a href="#reffn_4" title="Jump back to footnote [4] in the text."> &#8617;</a></p><p><a id="fn_5" class="headerlink" title="5"></a><sup>5</sup>. <a target="_blank" rel="noopener" href="https://github.com/scikit-learn/scikit-learn/blob/108486011b833cef8061615d926e2883941b9123/sklearn/tree/_tree.pyx#L1114">https://github.com/scikit-learn/scikit-learn/blob/108486011b833cef8061615d926e2883941b9123/sklearn/tree/_tree.pyx#L1114</a><a href="#reffn_5" title="Jump back to footnote [5] in the text."> &#8617;</a></p><p><a id="fn_6" class="headerlink" title="6"></a><sup>6</sup>. <a target="_blank" rel="noopener" href="https://github.com/slundberg/shap">https://github.com/slundberg/shap</a><a href="#reffn_6" title="Jump back to footnote [6] in the text."> &#8617;</a></p></div><hr><h3><span class="iconfont icon-history" style="padding-right:5px;font-size:1em"></span>修订记录</h3><div class="post-history-list"><ol reversed><li class="post-history-item">2023年5月30日 创建文章</li></ol></div><div class="post-footer"><hr><div class="next-post"><span class="iconfont icon-arrow-left-circle"></span> <a href="/String/AC/">AC 自动机入门</a></div><div class="prev-post"><a href="/Randomized/Summary/">随机算法总结 </a><span class="iconfont icon-arrow-right-circle"></span></div></div></article></main></body><div class="side-button"><div id="comment-button" class="button" title="Valine Comment"><span class="iconfont icon-comment"></span></div><div id="darkmode-button" class="button" title="Switch between day and night"><span class="iconfont icon-moonbyueliang"></span></div><div id="top" class="button" title="Back to top"><span class="iconfont icon-arrowup"></span></div></div><footer id="footer" style="margin:3em 0 2em 0;text-align:center;line-height:1.4em"><span class="post-wordcount iconfont icon-text"></span> 全站约 404,069 字<div class="license" style="padding:0 10px"><span>本站所有文章遵循许可协议 <a target="_blank" rel="license noreferrer noopener" href="https://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a></span></div><div class="credit"><span>Themed <a href="https://github.com/sshwy/hexo-theme-essence" target="_blank" rel="noreferrer noopener">Essence v1.9.11</a> | Powered by <a href="http://hexo.io" target="_blank" rel="noreferrer noopener">Hexo</a></span></div><div class="friends"><span class="friend-link-span">友情链接 </span><span class="icon friendsep"></span> <a target="_blank" rel="noopener" href="https://carol-xrl.github.io/"><span class="friend-link-span">Carol </span></a><span class="icon friendsep"></span> <a target="_blank" rel="noopener" href="https://www.cnblogs.com/GavinZheng"><span class="friend-link-span">GavinZheng </span></a><span class="icon friendsep"></span> <a target="_blank" rel="noopener" href="https://www.cnblogs.com/xryjr233"><span class="friend-link-span">xryjr233 </span></a><span class="icon friendsep"></span> <a target="_blank" rel="noopener" href="https://www.cnblogs.com/dysyn1314"><span class="friend-link-span" title="为什么我的头常常变绿？因为…… "><span style="font-weight:700"><span class="codeforces p">dy</span><span class="codeforces m">syn</span><span class="codeforces p">1314</span></span> </span></a><span class="icon friendsep"></span> <a target="_blank" rel="noopener" href="https://blog.aor.sd.cn"><span class="friend-link-span">RainAir </span></a><span class="icon friendsep"></span> <a target="_blank" rel="noopener" href="https://www.cnblogs.com/lcyfrog"><span class="friend-link-span">lcyfrog </span></a><span class="icon friendsep"></span> <a target="_blank" rel="noopener" href="https://www.cnblogs.com/maoyiting"><span class="friend-link-span">/hanx/se</span></a></div><div class="copyright"><span>Copyright &copy; 2019-present - Sshwy</span></div></footer><script src="/js/ssimple.js?20250423.js"></script><script src="https://pv.sohu.com/cityjson?ie=utf-8.js" defer></script></html>